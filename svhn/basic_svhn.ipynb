{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Python 2.7\n",
    "\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline \n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "print(sys.version)\n",
    "\n",
    "import os\n",
    "import cPickle \n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne \n",
    "import lasagne.layers as L\n",
    "import parmesan\n",
    "import cPickle as pickle\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tools as tls\n",
    "from data_loaders import svhn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### GLOBAL PARAMETERS ###\n",
    "plot_train   = True\n",
    "using_shared = False\n",
    "\n",
    "### META - HOW THE PROGRAM WORKS\n",
    "\n",
    "np.random.seed(1234) # reproducibility\n",
    "\n",
    "\n",
    "### CONSTANTS\n",
    "dataset = 'MNIST'\n",
    "print('dataset = {}'.format(dataset))\n",
    "\n",
    "if dataset == 'SVHN':    \n",
    "    file_name = 'data_no_share_c3' # assumes '.pkl'\n",
    "    IMG_LEN = 32\n",
    "    IMG_DEPTH = 3\n",
    "    cmap = None\n",
    "elif dataset == 'MNIST':\n",
    "    IMG_LEN = 28\n",
    "    IMG_DEPTH = 1\n",
    "    cmap = 'gray'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### PLOT SETTINGS\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "font_size = 15\n",
    "plt.rc('font',   size=font_size)       # controls default text sizes\n",
    "plt.rc('axes',   titlesize=font_size)  # fontsize of the axes title\n",
    "plt.rc('axes',   labelsize=font_size)  # fontsize of the x any y labels\n",
    "plt.rc('xtick',  labelsize=font_size)  # fontsize of the tick labels\n",
    "plt.rc('ytick',  labelsize=font_size)  # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=font_size)   # legend fontsize\n",
    "plt.rc('figure', titlesize=font_size)  # # size of the figure title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "### LOAD DATA\n",
    "\n",
    "if dataset == 'SVHN':\n",
    "    full_path = os.path.join(os.getcwd(), 'data')\n",
    "    full_path = os.path.join(full_path, file_name)\n",
    "    full_path  += '.pkl'\n",
    "    print(full_path)\n",
    "\n",
    "    with open(full_path, 'rb') as f:\n",
    "        x_trai, t_trai, x_vali, t_vali, x_test, t_test = pickle.load(f)\n",
    "\n",
    "    x_trai = x_trai/255\n",
    "    x_vali = x_vali/255\n",
    "    x_test = x_test/255\n",
    "\n",
    "\n",
    "elif dataset == 'MNIST':\n",
    "    full_path = os.path.join(os.getcwd(), 'data')\n",
    "    full_path = os.path.join(full_path  , 'mnist.npz')\n",
    "\n",
    "    data = np.load(full_path)\n",
    "    num_classes = 10\n",
    "    x_trai = data['X_train'].astype('float32')\n",
    "    t_trai = data['y_train'].astype('int32')\n",
    "\n",
    "    x_vali = data['X_valid'].astype('float32')\n",
    "    t_vali = data['y_valid'].astype('int32')\n",
    "\n",
    "    x_test = data['X_test'].astype('float32')\n",
    "    t_test = data['y_test'].astype('int32')\n",
    "\n",
    "\n",
    "    t_trai = tls.onehot(t_trai, 10)\n",
    "    t_vali = tls.onehot(t_vali, 10)\n",
    "    t_test = tls.onehot(t_test, 10)\n",
    "\n",
    "    \n",
    "    \n",
    "print('Size of total dataset: {:.2f} MB'.format(\n",
    "        (\n",
    "              sys.getsizeof(x_trai)\n",
    "            + sys.getsizeof(t_trai)\n",
    "            + sys.getsizeof(x_vali)\n",
    "            + sys.getsizeof(t_vali)\n",
    "            + sys.getsizeof(x_test)\n",
    "            + sys.getsizeof(t_test)\n",
    "        )/1.0e6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### CHECK DATA\n",
    "num_classes = t_trai.shape[1]\n",
    "print('Number of classes {}'.format(num_classes))\n",
    "\n",
    "num_features = x_trai[0].shape[0]\n",
    "print('Number of features {}'.format(num_features))\n",
    "\n",
    "print('')\n",
    "print('Train shape: ', \n",
    "      x_trai.shape, t_trai.shape)\n",
    "\n",
    "print('Valid shape: ', \n",
    "      x_vali.shape, t_vali.shape)\n",
    "\n",
    "print('Test shape:  ', \n",
    "      x_test.shape, t_test.shape)\n",
    "\n",
    "print('{}'.format(type(x_trai)))\n",
    "print('{}'.format(type(x_vali)))\n",
    "print('{}'.format(type(x_test)))\n",
    "print('')\n",
    "\n",
    "print('Prior')\n",
    "print(np.sum(t_trai, axis=0)/t_trai.shape[0])\n",
    "print(np.sum(t_test, axis=0)/t_test.shape[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### More data checks\n",
    "# Ensure that we have one hot encoding, and that the \n",
    "print(t_vali[:10,:])\n",
    "print(np.where(t_trai == 1)[1][:10])\n",
    "\n",
    "# If you don't get a 10xNUM_CLASS matrix, and a list with the index of the 1 of\n",
    "# each row, something went wrong.\n",
    "\n",
    "\n",
    "### Ensure that the data is scalled appropriately (between 0 and 1)\n",
    "\n",
    "print('\\nMaximum training value: {}'.format(np.max(x_trai)))\n",
    "if np.max(x_trai) > 1:\n",
    "    print('WARNING!! Maximum value in input data is {}'.format(np.max(x_trai)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### VISUALIZE \n",
    "canv, lab = tls.plot_svhn(x_trai, t_trai, t=6, IMG_LEN=IMG_LEN, \n",
    "                          IMG_DEPTH=IMG_DEPTH, cmap=cmap)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.imshow(canv, cmap=cmap, interpolation='nearest')\n",
    "\n",
    "ax.set_title('Data visualization')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(lab) # The labels associated with the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### HYPER PARAMETERS\n",
    "# VOLATILE HP\n",
    "# learning_rate = 1e-5 # log_bernoulli\n",
    "learning_rate = 1e-4 # squared\n",
    "KL_coef = 0.5\n",
    "L1 = 0\n",
    "L2 = 0\n",
    "\n",
    "samples_to_process = 1e10\n",
    "val_interval       = 1e5\n",
    "batch_size         = 1e3\n",
    "\n",
    "if not val_interval/batch_size % 1 == 0:\n",
    "    print('WARNING: val_interval must be divisible by batch_size')\n",
    "    print('Validation will be performed rarely with current settings')\n",
    "\n",
    "\n",
    "# ARCHITECTURE\n",
    "num_latent_1 = num_classes*10\n",
    "hid_size = 1000\n",
    "\n",
    "\n",
    "# STABLE HP\n",
    "eq_size = 1\n",
    "iw_size = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### HELPER FUNCTIONS\n",
    "from lasagne.objectives import squared_error\n",
    "\n",
    "# c = -0.5 * np.log(2*np.pi)\n",
    "clip = lambda x: T.clip(x,-10,10) #used to limit the variance (why?)\n",
    "\n",
    "def log_bernoulli(x, p, eps=1e-7):\n",
    "    \"\"\"\n",
    "    Computes the binary cross-entropy between a target and \n",
    "\n",
    "    Use eps if you don't want to alow values ==0, ==1\n",
    "    \"\"\"\n",
    "\n",
    "    p = T.clip(p, eps, 1.0 - eps)\n",
    "    return -T.nnet.binary_crossentropy(p, x)\n",
    "\n",
    "\n",
    "\n",
    "def kl_normal_2_stdnormal(mu, lv):\n",
    "    \"\"\"Compute the KL divergence from the standard normal dist\"\"\"\n",
    "    return - 0.5 * (1 + lv - mu**2 - T.exp(lv))\n",
    "\n",
    "\n",
    "def LogLikelihood(mux, x, muq, lvq):\n",
    "    \"\"\"\n",
    "    Compute the mean const of a set of examples\n",
    "    \"\"\"\n",
    "    #Sum over the latent dimension, mean over the the samples\n",
    "    reconstruction_cost = - squared_error(x, mux).sum(axis=1).mean()\n",
    "#     reconstruction_cost = log_bernoulli(x, mux).sum(axis=1).mean()\n",
    "    KL_qp = kl_normal_2_stdnormal(muq, lvq).sum(axis=1).mean()\n",
    "    KL_qp *= KL_coef\n",
    "        \n",
    "    LL = (reconstruction_cost - KL_qp)/(IMG_LEN*IMG_LEN) \n",
    "#     print('{:10.0f}:{:10.0f}:{:10.0f}'.format(LL, reconstruction_cost, KL_qp))\n",
    "    print('{}:{}:{}'.format(LL, reconstruction_cost, KL_qp))\n",
    "    \n",
    "    return LL, reconstruction_cost, KL_qp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "### CREATE MODEL\n",
    "from lasagne.nonlinearities import leaky_rectify, rectify, sigmoid\n",
    "from parmesan.layers import SampleLayer\n",
    "\n",
    "\n",
    "### ENCODER\n",
    "l_in_x   = L.InputLayer(shape=(None, num_features), name='l_in_x')\n",
    "l_in_norm= L.BatchNormLayer(l_in_x)\n",
    "\n",
    "l_en_1   = L.DenseLayer(l_in_norm, \n",
    "                        num_units=hid_size,\n",
    "                        nonlinearity=rectify,\n",
    "                        name='l_en_1')\n",
    "# l_en_2   = L.DenseLayer(l_en_1,\n",
    "#                         num_units=hid_size,\n",
    "#                         nonlinearity=rectify,\n",
    "#                         name='l_en_2')\n",
    "\n",
    "### Create latent parameters\n",
    "l_mu_1   = L.DenseLayer(l_en_1,\n",
    "                        num_units=num_latent_1,\n",
    "                        nonlinearity=None,\n",
    "                        name='l_mu_1')\n",
    "l_lv_1   = L.DenseLayer(l_en_1,\n",
    "                        num_units=num_latent_1,\n",
    "                        nonlinearity=clip,\n",
    "                        name='l_lv_1')\n",
    "\n",
    "### sample a latent representation:\n",
    "#    z ~ q(z|x) = N(mu(x), logvar(x)\n",
    "l_z_1      = SampleLayer(mean=l_mu_1, \n",
    "                         log_var=l_lv_1, \n",
    "                         eq_samples=eq_size, \n",
    "                         iw_samples=iw_size, \n",
    "                         name='l_z_1')\n",
    "\n",
    "### DECODER\n",
    "l_in_z   = L.InputLayer(shape=(None, num_latent_1), \n",
    "                        name = 'l_in_z')\n",
    "l_dec_1  = L.DenseLayer(l_in_z, \n",
    "                        num_units = hid_size,\n",
    "                        nonlinearity = rectify,\n",
    "                        name = 'l_dec_1')\n",
    "# l_dec_2  = L.DenseLayer(l_dec_1, \n",
    "#                         num_units = hid_size,\n",
    "#                         nonlinearity = rectify,\n",
    "#                        name='l_dec_2')\n",
    "\n",
    "# Sigmoid is used because the original images are $\\in [0,1]$\n",
    "l_out    = L.DenseLayer(l_dec_1, \n",
    "                        num_units=num_features,\n",
    "                        nonlinearity=sigmoid,\n",
    "                        name='l_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "### CREATE INTERFACE VARIABLES\n",
    "\n",
    "sym_x = T.matrix('x') # (batch_size x 3072)\n",
    "sym_z = T.matrix('z') # Latent variable (batch_size x num_latent)\n",
    "\n",
    "# Training variables\n",
    "z_train, mu_train, lv_train = L.get_output([l_z_1, l_mu_1, l_lv_1],\n",
    "                                           {l_in_x:sym_x}, \n",
    "                                           deterministic = False)\n",
    "out_train                   = L.get_output(l_out,\n",
    "                                           {l_in_z:z_train}, \n",
    "                                           deterministic = False)\n",
    "\n",
    "# Test variables\n",
    "z_eval, mu_eval, lv_eval    = L.get_output([l_z_1, l_mu_1, l_lv_1],\n",
    "                                           {l_in_x:sym_x},\n",
    "                                           deterministic = True)\n",
    "out_eval                    = L.get_output(l_out,\n",
    "                                           {l_in_z:z_eval}, \n",
    "                                           deterministic = True)\n",
    "\n",
    "# For generating artificial data (samples)\n",
    "mux_sample               = L.get_output(l_out, {l_in_z: sym_z})\n",
    "\n",
    "# Copute the cost\n",
    "LL_train, log_px_train, KL_train = \\\n",
    "    LogLikelihood(out_train, sym_x, mu_train, lv_train)\n",
    "\n",
    "LL_eval, log_px_eval, KL_eval = \\\n",
    "    LogLikelihood(out_eval, sym_x, mu_eval, lv_eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "### CREATE TRAINING FUNCTIONS\n",
    "\n",
    "all_params = L.get_all_params([l_z_1, l_out], trainable=True)\n",
    "all_grads  = T.grad(-LL_train, all_params)\n",
    "\n",
    "updates    = lasagne.updates.adam(all_grads, all_params,\n",
    "                                  learning_rate=learning_rate)\n",
    "\n",
    "# Training function: Return loss, and update weights\n",
    "f_train = theano.function(inputs=[sym_x],\n",
    "                          outputs=[LL_train, log_px_train, KL_train],\n",
    "                          updates=updates)\n",
    "\n",
    "# Evaluation function: Return loss\n",
    "f_eval = theano.function(inputs=[sym_x],\n",
    "                         outputs=[LL_train, log_px_train, KL_train])\n",
    "\n",
    "# Get latent variable values\n",
    "f_z              = theano.function(inputs=[sym_x], outputs=[z_eval])\n",
    "\n",
    "# Return the reconstruction\n",
    "f_reconstruction = theano.function(inputs=[sym_x], outputs=[out_eval])\n",
    "\n",
    "# Simulate artificial data, given an artificial latent variable\n",
    "f_simulate = theano.function(inputs=[sym_z], outputs=[mux_sample])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "### TRAINING\n",
    "from IPython.display import Image, display, clear_output\n",
    "import sklearn.decomposition\n",
    "pca = sklearn.decomposition.PCA(n_components=2)\n",
    "\n",
    "\n",
    "\n",
    "LL_train, KL_train, logpx_train = [],[],[]\n",
    "LL_valid, KL_valid, logpx_valid = [],[],[]\n",
    "samples_processed = 0\n",
    "\n",
    "valid_samples_processed = []\n",
    "\n",
    "\n",
    "try:\n",
    "    while samples_processed < samples_to_process:\n",
    "#         _LL_train, _KL_train, _logpx_train = [],[],[]\n",
    "        idxs = np.random.choice(\n",
    "                range(x_trai.shape[0]), \n",
    "                size=(batch_size), \n",
    "                replace=False)\n",
    "    \n",
    "        out = f_train(x_trai[idxs])\n",
    "        samples_processed += batch_size\n",
    "            \n",
    "        if samples_processed % val_interval == 0:\n",
    "            print('{:10.0f} sampeles processed.'.format(samples_processed))\n",
    "            print('{:10.0f} sampeles in total.'.format(samples_to_process))\n",
    "            valid_samples_processed += [samples_processed]\n",
    "\n",
    "            # Run through training set\n",
    "            out = f_eval(x_trai[:10000]) # TAKES A LOT OF MEMORY!\n",
    "            LL_train    += [out[0]]\n",
    "            logpx_train += [out[1]]\n",
    "            KL_train    += [out[2]]\n",
    "            \n",
    "            # Run through validation set\n",
    "            out = f_eval(x_vali)\n",
    "            LL_valid    += [out[0]]\n",
    "            logpx_valid += [out[1]]\n",
    "            KL_valid    += [out[2]]\n",
    "            \n",
    "            # Compute latent variables\n",
    "            z_eval = f_z(x_vali)[0]\n",
    "            \n",
    "            # Create samples, sampling from prior distribution (std. normal)\n",
    "            x_sample = f_simulate(np.random.normal(size=(100, num_latent_1)).astype('float32'))[0]\n",
    "#             x_sample = f_sample(np.random.normal(size=(100, num_latent_1 * size_up_factor)).astype('float32'))[0]\n",
    "\n",
    "            \n",
    "            # Create reconstruction\n",
    "            x_recon = f_reconstruction(x_vali)[0]\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            if plot_train:\n",
    "#             if False:\n",
    "                _, ax_learn = plt.subplots(figsize=(7, 7), dpi=50)\n",
    "\n",
    "                ### Plot learning curves \n",
    "                plt.subplot(2,2,1)\n",
    "                plt.xlabel('Updates')\n",
    "                plt.plot(valid_samples_processed, LL_train, \n",
    "                         color=\"blue\", label='LL')\n",
    "                plt.plot(valid_samples_processed, logpx_train, \n",
    "                         color=\"red\", label='log(p(x))')\n",
    "                plt.plot(valid_samples_processed, LL_valid, \n",
    "                         color=\"blue\", \n",
    "                         linestyle=\"--\")\n",
    "                plt.plot(valid_samples_processed, logpx_valid,\n",
    "                         color=\"red\", \n",
    "                         linestyle=\"--\")\n",
    "                plt.legend(loc=2)\n",
    "                plt.title('-train, --valid')\n",
    "                plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "                plt.grid('on')\n",
    "                \n",
    "                ### Plot PCA of latent space\n",
    "                plt.subplot(2,2,2)\n",
    "                plt.cla()\n",
    "                plt.xlabel('PCA 0'), plt.ylabel('PCA 1')\n",
    "                color = iter(plt.get_cmap('brg')(np.linspace(0, 1.0, num_classes)))\n",
    "                for i in range(num_classes):\n",
    "                    clr = next(color)\n",
    "                    pca_trans = pca.fit_transform(z_eval)\n",
    "                    plt.scatter(pca_trans[tls.onehot2int(t_vali)==i, 0], \n",
    "                                pca_trans[tls.onehot2int(t_vali)==i, 1], \n",
    "                                c=clr, \n",
    "                                s=5., \n",
    "                                lw=0, \n",
    "                                marker='o', )\n",
    "                plt.grid('on')\n",
    "\n",
    "#                 plt.savefig(\"out52.png\")\n",
    "#                 display(Image(filename=\"out52.png\"))\n",
    "\n",
    "                ### PLOT KL term\n",
    "                plt.subplot(2,2,3)\n",
    "                plt.xlabel('Updates')\n",
    "                plt.plot(valid_samples_processed, KL_train, color=\"blue\")\n",
    "                plt.plot(valid_samples_processed, KL_valid, color=\"blue\", linestyle=\"--\")\n",
    "                plt.legend(['KL(q||p)'])\n",
    "                plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "                # Formatting\n",
    "                plt.grid('on')\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                ### Plot samples                \n",
    "                plt.subplot(2,2,4)\n",
    "                sample, _ = tls.plot_svhn(x_sample, t=6, cmap=cmap,\n",
    "                    IMG_LEN=IMG_LEN, IMG_DEPTH=IMG_DEPTH)\n",
    "\n",
    "                \n",
    "                ## PLOT images and their reconstructions\n",
    "                orig_all = []\n",
    "                recon_all= []\n",
    "                for c in range(num_classes):\n",
    "                    orig, _ = tls.plot_svhn(x_vali[tls.onehot2int(t_vali)==c], \n",
    "                                             t=6, cmap=cmap, IMG_LEN=IMG_LEN, \n",
    "                                             IMG_DEPTH=IMG_DEPTH)\n",
    "                    orig_all.append(orig)\n",
    "\n",
    "                    recon, _ = tls.plot_svhn(x_recon[tls.onehot2int(t_vali)==c],\n",
    "                                             t=6, cmap=cmap, IMG_LEN=IMG_LEN, \n",
    "                                             IMG_DEPTH=IMG_DEPTH)\n",
    "                    recon_all.append(recon)\n",
    "        \n",
    "        \n",
    "            # Clear previous plots\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            plt.imshow(sample, cmap=cmap, interpolation='None')\n",
    "            plt.title('Samples')\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            for c in range(num_classes):\n",
    "                _, ax = plt.subplots(1,2)\n",
    "\n",
    "                ax[0].imshow(orig_all[c], cmap=cmap, interpolation='None')\n",
    "                ax[0].set_title('Original')\n",
    "                ax[0].axis('off')\n",
    "                \n",
    "                ax[1].imshow(recon_all[c], cmap=cmap, interpolation='None')\n",
    "                ax[1].set_title('Reconstruction')\n",
    "                ax[1].axis('off')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_sample = f_simulate(np.random.normal(size=(100, num_latent_1)).astype('float32'))[0]\n",
    "\n",
    "print(x_sample.shape)\n",
    "# print(x_sample[0])\n",
    "print(max(x_sample[0]))\n",
    "\n",
    "\n",
    "\n",
    "canv, _ = tls.plot_svhn(x_sample, t=6, cmap=cmap,\n",
    "    IMG_LEN=IMG_LEN, IMG_DEPTH=IMG_DEPTH)\n",
    "\n",
    "plt.imshow(canv, cmap=cmap)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fig, ax_sam = plt.subplots()\n",
    "\n",
    "# x_sample.shape\n",
    "\n",
    "# ax_sam.cla()\n",
    "# plt.title('Samples')\n",
    "# plt.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "# idx = 0\n",
    "# canvas = np.zeros((IMG_LEN*10, 10*IMG_LEN, ))\n",
    "# ## Plot generated samples\n",
    "# for i in range(10):\n",
    "#     for j in range(10):\n",
    "#         if dataset == 'SVHN':\n",
    "#             canvas[i*IMG_LEN:(i+1)*IMG_LEN, j*IMG_LEN:(j+1)*IMG_LEN, :] = \\\n",
    "#                 x_sample[idx].reshape((IMG_LEN, IMG_LEN, 3))\n",
    "#         elif dataset == 'MNIST':\n",
    "#             canvas[i*IMG_LEN:(i+1)*IMG_LEN, j*IMG_LEN:(j+1)*IMG_LEN] = \\\n",
    "#                 x_sample[idx].reshape((IMG_LEN, IMG_LEN))                            \n",
    "#         idx += 1\n",
    "# plt.imshow(canvas)\n",
    "# plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

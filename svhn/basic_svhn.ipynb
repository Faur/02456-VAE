{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Python 2.7\n",
    "\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline \n",
    "\n",
    "import sys\n",
    "sys.path\n",
    "sys.path.append('..')\n",
    "print(sys.version)\n",
    "\n",
    "import os\n",
    "import cPickle \n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne \n",
    "import lasagne.layers as L\n",
    "import parmesan\n",
    "import cPickle as pickle\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tools as tls\n",
    "\n",
    "\n",
    "from data_loaders import svhn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### GLOBAL PARAMETERS ###\n",
    "\n",
    "### META - HOW THE PROGRAM WORKS\n",
    "file_name = 'data_c3'\n",
    "\n",
    "np.random.seed(1234) # reproducibility\n",
    "\n",
    "\n",
    "### CONSTANTS\n",
    "IMG_LEN = 32\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "### LOAD DATA\n",
    "\n",
    "\n",
    "full_path = os.path.join(os.getcwd(), 'data')\n",
    "full_path = os.path.join(full_path, file_name)\n",
    "full_path  += '.pkl'\n",
    "print(full_path)\n",
    "\n",
    "with open(full_path, 'rb') as f:\n",
    "    x_trai, t_trai, x_vali, t_vali, x_test, t_test = pickle.load(f)\n",
    "\n",
    "print('Size of total dataset: {:.2f} MB'.format(\n",
    "        (\n",
    "              sys.getsizeof(x_trai.get_value())\n",
    "            + sys.getsizeof(t_trai.eval())\n",
    "            + sys.getsizeof(x_vali.get_value())\n",
    "            + sys.getsizeof(t_vali.eval())\n",
    "            + sys.getsizeof(x_test.get_value())\n",
    "            + sys.getsizeof(t_test.eval())\n",
    "        )/1.0e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### CHECK DATA\n",
    "num_classes = np.unique(np.where(t_trai.eval() == 1)[1]).shape[0]\n",
    "print('Number of classes {}'.format(num_classes))\n",
    "\n",
    "num_features = x_trai.get_value()[0].shape[0]\n",
    "print('Number of features {}'.format(num_features))\n",
    "\n",
    "print('')\n",
    "print('Train shape: ', \n",
    "      x_trai.get_value().shape, t_trai.eval().shape)\n",
    "\n",
    "print('Valid shape: ', \n",
    "      x_vali.get_value().shape, t_vali.eval().shape)\n",
    "\n",
    "print('Test shape:  ', \n",
    "      x_test.get_value().shape, t_test.eval().shape)\n",
    "\n",
    "print('{}'.format(type(x_trai)))\n",
    "print('{}'.format(type(x_vali)))\n",
    "print('{}'.format(type(x_test)))\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### VISUALIZE \n",
    "\n",
    "# TODO: WHEN normalize/renormalize are made DO add color to this function\n",
    "\n",
    "tls.plot_svhn(x_trai.get_value(), t=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### HYPER PARAMETERS\n",
    "# VOLATILE HP\n",
    "learning_rate = 1e-2\n",
    "L1 = 0\n",
    "L2 = 0\n",
    "\n",
    "\n",
    "# ARCHITECTURE\n",
    "num_latent_1 = num_classes\n",
    "hid_size = 500\n",
    "\n",
    "\n",
    "# STABLE HP\n",
    "eq_size = 1\n",
    "iw_size = 1\n",
    "batch_size = 128\n",
    "max_epoch = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### HELPER FUNCTIONS\n",
    "from lasagne.objectives import squared_error\n",
    "\n",
    "# c = -0.5 * np.log(2*np.pi)\n",
    "clip = lambda x: T.clip(x,-10,10) #used to limit the variance (why?)\n",
    "\n",
    "def log_bernoulli(x, p, eps=1e-32):\n",
    "    \"\"\"\n",
    "    Computes the binary cross-entropy between a target and \n",
    "\n",
    "    Use eps if you don't want to alow values ==0, ==1\n",
    "    \"\"\"\n",
    "\n",
    "    p = T.clip(p, eps, 1.0 - eps)\n",
    "    return -T.nnet.binary_crossentropy(p, x)\n",
    "\n",
    "\n",
    "\n",
    "def kl_normal_2_stdnormal(mu, lv):\n",
    "    \"\"\"Compute the KL divergence from the standard normal dist\"\"\"\n",
    "    return - 0.5 * (1 + lv - mu**2 - T.exp(lv))\n",
    "\n",
    "\n",
    "def LogLikelihood(mux, x, muq, lvq):\n",
    "    \"\"\"\n",
    "    Compute the cost of the network, using \n",
    "    \"\"\"\n",
    "    #Sum over the latent dimension, mean over the the samples\n",
    "    reconstruction_cost = squared_error(x, mux).sum(axis=1).mean()\n",
    "    KL_qp = kl_normal_2_stdnormal(muq, lvq).sum(axis=1).mean()\n",
    "    \n",
    "    LL = reconstruction_cost - KL_qp\n",
    "    \n",
    "    return LL, reconstruction_cost, KL_qp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "### CREATE MODEL\n",
    "from lasagne.nonlinearities import leaky_rectify, rectify, sigmoid\n",
    "from parmesan.layers import SampleLayer\n",
    "\n",
    "\n",
    "### ENCODER\n",
    "l_in_x   = L.InputLayer(shape=(None, num_features), name='l_in_x')\n",
    "\n",
    "l_en_1   = L.DenseLayer(l_in_x, \n",
    "                        num_units=hid_size,\n",
    "                        nonlinearity=rectify,\n",
    "                        name='l_en_1')\n",
    "l_en_2   = L.DenseLayer(l_en_1,\n",
    "                        num_units=hid_size,\n",
    "                        nonlinearity=rectify,\n",
    "                        name='l_en_2')\n",
    "\n",
    "# Create latent parameters\n",
    "l_mu_1   = L.DenseLayer(l_en_2,\n",
    "                        num_units=num_latent_1,\n",
    "                        nonlinearity=None,\n",
    "                        name='l_mu_1')\n",
    "l_lv_1   = L.DenseLayer(l_en_2,\n",
    "                        num_units=num_latent_1,\n",
    "                        nonlinearity=clip,\n",
    "                        name='l_lv_1')\n",
    "\n",
    "# sample a latent representation:\n",
    "#    z ~ q(z|x) = N(mu(x), logvar(x)\n",
    "l_z_1      = SampleLayer(mean=l_mu_1, \n",
    "                         log_var=l_lv_1, \n",
    "                         eq_samples=eq_size, \n",
    "                         iw_samples=iw_size, \n",
    "                         name='l_z_1')\n",
    "\n",
    "### DECODER\n",
    "l_in_z   = L.InputLayer(shape=(None, num_latent_1), \n",
    "                        name = 'l_in_z')\n",
    "l_dec_1  = L.DenseLayer(l_in_z, \n",
    "                        num_units = hid_size,\n",
    "                        nonlinearity = rectify,\n",
    "                        name = 'l_dec_1')\n",
    "l_dec_2  = L.DenseLayer(l_dec_1, \n",
    "                        num_units = hid_size,\n",
    "                        nonlinearity = rectify,\n",
    "                       name='l_dec_2')\n",
    "\n",
    "# Sigmoid is used because the original images are $\\in [0,1]$\n",
    "l_out    = L.DenseLayer(l_dec_2, \n",
    "                        num_units=num_features,\n",
    "                        nonlinearity=sigmoid,\n",
    "                        name='l_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "### CREATE INTERFACE VARIABLES\n",
    "\n",
    "sym_x = T.matrix('x') # (batch_size x 3072)\n",
    "sym_z = T.matrix('z') # Latent variable (batch_size x num_latent)\n",
    "\n",
    "# Training variables\n",
    "z_train, mu_train, lv_train = L.get_output([l_z_1, l_mu_1, l_lv_1],\n",
    "                                           {l_in_x:sym_x}, \n",
    "                                           deterministic = False)\n",
    "out_train                   = L.get_output(l_out,\n",
    "                                           {l_in_z:z_train}, \n",
    "                                           deterministic = False)\n",
    "\n",
    "# Test variables\n",
    "z_eval, mu_eval, lv_eval    = L.get_output([l_z_1, l_mu_1, l_lv_1],\n",
    "                                           {l_in_x:sym_x},\n",
    "                                           deterministic = True)\n",
    "out_eval                    = L.get_output(l_out,\n",
    "                                           {l_in_z:z_eval}, \n",
    "                                           deterministic = True)\n",
    "\n",
    "# For generating artificial data (samples)\n",
    "mux_sample               = L.get_output(l_out, {l_in_z: sym_z})\n",
    "\n",
    "# Copute the cost\n",
    "LL_train, log_px_train, KL_train = \\\n",
    "    LogLikelihood(out_train, sym_x, mu_train, lv_train)\n",
    "\n",
    "LL_eval, log_px_eval, KL_eval = \\\n",
    "    LogLikelihood(out_eval, sym_x, mu_eval, lv_eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "### CREATE TRAINING FUNCTIONS\n",
    "\n",
    "all_params = L.get_all_params([l_z_1, l_out], trainable=True)\n",
    "all_grads  = T.grad(-LL_train, all_params)\n",
    "\n",
    "updates    = lasagne.updates.adam(all_grads, all_params,\n",
    "                                  learning_rate=learning_rate)\n",
    "\n",
    "# Training function: Return loss, and update weights\n",
    "f_train = theano.function(inputs=[sym_x],\n",
    "                          outputs=[LL_train, log_px_train, KL_train],\n",
    "                          updates=updates)\n",
    "\n",
    "# Evaluation function: Return loss\n",
    "f_eval = theano.function(inputs=[sym_x],\n",
    "                         outputs=[LL_train, log_px_train, KL_train])\n",
    "\n",
    "# Get latent variable values\n",
    "f_z              = theano.function(inputs=[sym_x], outputs=[z_eval])\n",
    "\n",
    "# Return the reconstruction\n",
    "f_reconstruction = theano.function(inputs=[sym_x], outputs=[out_eval])\n",
    "\n",
    "# Simulate artificial data, given an artificial latent variable\n",
    "f_simulate = theano.function(inputs=[sym_z], outputs=[mux_sample])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

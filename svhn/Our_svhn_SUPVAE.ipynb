{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Python 2.7\n",
    "\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline \n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "print(sys.version)\n",
    "\n",
    "import os\n",
    "import cPickle \n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne \n",
    "import lasagne.layers as L\n",
    "import parmesan\n",
    "import cPickle as pickle\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tools as tls\n",
    "from data_loaders import svhn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Install Parmasan](https://github.com/casperkaae/parmesan)\n",
    "\n",
    "    git clone https://github.com/casperkaae/parmesan.git\n",
    "    cd parmesan\n",
    "    python setup.py develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### GLOBAL PARAMETERS ###\n",
    "plot_train   = True\n",
    "using_shared = False\n",
    "\n",
    "### META - HOW THE PROGRAM WORKS\n",
    "\n",
    "np.random.seed(1234) # reproducibility\n",
    "\n",
    "\n",
    "### CONSTANTS\n",
    "dataset = 'MNIST'\n",
    "#dataset = 'SVHN'\n",
    "print('dataset = {}'.format(dataset))\n",
    "\n",
    "if dataset == 'SVHN':    \n",
    "    file_name = 'data_svhn_c3' # assumes '.pkl'\n",
    "    IMG_LEN = 32\n",
    "    IMG_DEPTH = 3\n",
    "    cmap = None\n",
    "elif dataset == 'MNIST':\n",
    "    IMG_LEN = 28\n",
    "    IMG_DEPTH = 1\n",
    "    cmap = 'gray'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### PLOT SETTINGS\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "font_size = 12\n",
    "plt.rc('font',   size=font_size)       # controls default text sizes\n",
    "plt.rc('axes',   titlesize=font_size)  # fontsize of the axes title\n",
    "plt.rc('axes',   labelsize=font_size)  # fontsize of the x any y labels\n",
    "plt.rc('xtick',  labelsize=font_size)  # fontsize of the tick labels\n",
    "plt.rc('ytick',  labelsize=font_size)  # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=font_size)   # legend fontsize\n",
    "plt.rc('figure', titlesize=font_size)  # # size of the figure title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "### LOAD DATA\n",
    "\n",
    "if dataset == 'SVHN':\n",
    "    full_path = os.path.join(os.getcwd(), 'data')\n",
    "    full_path = os.path.join(full_path, file_name)\n",
    "    full_path  += '.pkl'\n",
    "    print(full_path)\n",
    "\n",
    "    with open(full_path, 'rb') as f:\n",
    "        x_trai, t_trai, x_vali, t_vali, x_test, t_test = pickle.load(f)\n",
    "\n",
    "    x_trai = x_trai/255\n",
    "    x_vali = x_vali/255\n",
    "    x_test = x_test/255\n",
    "\n",
    "\n",
    "elif dataset == 'MNIST':\n",
    "    full_path = os.path.join(os.getcwd(), 'data')\n",
    "    full_path = os.path.join(full_path  , 'mnist.npz')\n",
    "\n",
    "    data = np.load(full_path)\n",
    "    x_trai = data['X_train'].astype('float32')\n",
    "    t_trai = data['y_train'].astype('int32')\n",
    "\n",
    "    x_vali = data['X_valid'].astype('float32')\n",
    "    t_vali = data['y_valid'].astype('int32')\n",
    "\n",
    "    x_test = data['X_test'].astype('float32')\n",
    "    t_test = data['y_test'].astype('int32')\n",
    "\n",
    "\n",
    "    t_trai = tls.onehot(t_trai, 10)\n",
    "    t_vali = tls.onehot(t_vali, 10)\n",
    "    t_test = tls.onehot(t_test, 10)\n",
    "\n",
    "\n",
    "\n",
    "### CHECK DATA\n",
    "num_classes = t_trai.shape[1]\n",
    "print('Number of classes {}'.format(num_classes))\n",
    "\n",
    "num_features = x_trai[0].shape[0]\n",
    "print('Number of features {}'.format(num_features))\n",
    "\n",
    "print('')\n",
    "print('Train shape: ', \n",
    "      x_trai.shape, t_trai.shape)\n",
    "\n",
    "print('Valid shape: ', \n",
    "      x_vali.shape, t_vali.shape)\n",
    "\n",
    "print('Test shape:  ', \n",
    "      x_test.shape, t_test.shape)\n",
    "\n",
    "print('{}'.format(type(x_trai)))\n",
    "print('{}'.format(type(x_vali)))\n",
    "print('{}'.format(type(x_test)))\n",
    "print('')\n",
    "\n",
    "print('Prior (percent)')\n",
    "print(np.round(np.sum(t_trai, axis=0)/t_trai.shape[0]*1000)/10.)\n",
    "print(np.round(np.sum(t_test, axis=0)/t_test.shape[0]*1000)/10.)\n",
    "\n",
    "\n",
    "print('Size of total dataset: {:.2f} MB'.format(\n",
    "        (\n",
    "              sys.getsizeof(x_trai)\n",
    "            + sys.getsizeof(t_trai)\n",
    "            + sys.getsizeof(x_vali)\n",
    "            + sys.getsizeof(t_vali)\n",
    "            + sys.getsizeof(x_test)\n",
    "            + sys.getsizeof(t_test)\n",
    "        )/1.0e6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### LAZY REDUCTION OF DATA SET SIZE\n",
    "# x_trai = x_trai[:10000]\n",
    "# t_trai = t_trai[:10000]\n",
    "\n",
    "# x_trai = x_trai[:10000]\n",
    "# t_trai = t_trai[:10000]\n",
    "\n",
    "# x_trai = x_trai[:10000]\n",
    "# t_trai = t_trai[:10000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a subset of labeled data points\n",
    "\n",
    "num_labeled = 100 # per class\n",
    "if dataset=='MNIST':\n",
    "    num_labeled=10\n",
    "\n",
    "idxs_train_l = []\n",
    "for i in range(num_classes):\n",
    "    idxs = np.where(np.argmax(t_trai,axis=-1) == i)[0]\n",
    "    idxs_train_l += np.random.choice(idxs, size=num_labeled).tolist()\n",
    "\n",
    "x_train_l = x_trai[idxs_train_l].astype('float32')\n",
    "t_train_l = (t_trai[idxs_train_l,:]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### More data checks\n",
    "# Ensure that we have one hot encoding, and that the \n",
    "print('Label check')\n",
    "print(t_vali[:10,:])\n",
    "print(np.where(t_trai == 1)[1][:10])\n",
    "\n",
    "print(\"If you don't get a 10xNUM_CLASS matrix, and a list with the\",\n",
    "      \"index of the 1 of each row, something went wrong.\")\n",
    "\n",
    "print\n",
    "print('Input check')\n",
    "### Ensure that the data is scalled appropriately (between 0 and 1)\n",
    "\n",
    "print('\\nMaximum training value: {}'.format(np.max(x_trai)))\n",
    "if np.max(x_trai) > 1:\n",
    "    print('WARNING!! Maximum value in input data is {}'.format(np.max(x_trai)))\n",
    "else:\n",
    "    print('Input: fine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### VISUALIZE \n",
    "canv, lab = tls.plot_svhn(x_trai, t_trai, t=6, IMG_LEN=IMG_LEN, \n",
    "                          IMG_DEPTH=IMG_DEPTH, cmap=cmap)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.imshow(canv, cmap=cmap, interpolation='nearest')\n",
    "\n",
    "ax.set_title('Data visualization')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(lab) # The labels associated with the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### HYPER PARAMETERS\n",
    "# VOLATILE HP\n",
    "learning_rate = 1e-3\n",
    "smooth_factor=0\n",
    "L1 = 0\n",
    "L2 = 0#1e-4\n",
    "\n",
    "samples_to_process = 1e10\n",
    "val_interval=1e5\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "# ARCHITECTURE\n",
    "hid_size = 512\n",
    "fraction = 0.2\n",
    "num_latent_1 = round(hid_size*fraction)\n",
    "num_latent=num_latent_1\n",
    "#num_latent_1 = 300\n",
    "skip_connections=True\n",
    "deconv_rec=False\n",
    "augmented_train=False\n",
    "\n",
    "conv_net_class=True\n",
    "\n",
    "# STABLE HP\n",
    "eq_size = 1\n",
    "iw_size = 1\n",
    "\n",
    "# Weigth elements in likelihood \n",
    "Alpha_in=0.1\n",
    "Beta_in=1\n",
    "KL_weigth_sup_in=1\n",
    "KL_weigth_uns_in=1\n",
    "\n",
    "#Prior y\n",
    "# prior_y=np.mean(t_trai,axis=0).reshape(1,-1).astype('float32') # True prior is problematic\n",
    "prior_y = np.asarray([1/float(num_classes) for i in range(num_classes)]).astype('float32')#flat prior\n",
    "prior_y = prior_y.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prior_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### HELPER FUNCTIONS\n",
    "from lasagne.objectives import squared_error\n",
    "from lasagne.nonlinearities import leaky_rectify, rectify, sigmoid,softmax\n",
    "from parmesan.layers import SampleLayer\n",
    "\n",
    "def onehot(t, num_classes):\n",
    "    out = np.zeros((t.shape[0], num_classes)).astype('float32')\n",
    "    for row, col in enumerate(t):\n",
    "        out[row, col] = 1\n",
    "    return out\n",
    "\n",
    "\n",
    "# c = -0.5 * np.log(2*np.pi)\n",
    "clip = lambda x: T.clip(x,-10,10) #used to limit the variance (why?)\n",
    "\n",
    "def log_bernoulli(target, output, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Computes the binary cross-entropy between a target and \n",
    "\n",
    "    Use eps if you don't want to alow values ==0, ==1\n",
    "    \"\"\"\n",
    "\n",
    "#     output = T.clip(output, eps, 1.0 - eps)\n",
    "#     return -T.nnet.binary_crossentropy(output, target)\n",
    "    return (target * T.log(output + eps) + (1.0 - target) * T.log(1.0 - output + eps))\n",
    "#     return (target * T.log(output)       + (1.0 - target) * T.log(1.0 - output))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def kl_normal_2_stdnormal(mu, lv):\n",
    "    \"\"\"Compute the KL divergence from the standard normal dist\"\"\"\n",
    "    return - 0.5 * (1 + lv - mu**2 - T.exp(lv))\n",
    "\n",
    "\n",
    "# def ReconLogLikelihood(mux, x, muq, lvq,KL_weigth=1):\n",
    "#     \"\"\"\n",
    "#     Compute the cost of the network, using \n",
    "#     \"\"\"\n",
    "#     #Sum over the latent dimension, mean over the the samples\n",
    "#     #reconstruction_cost = -squared_error(x, mux).sum(axis=1).mean()\n",
    "#     reconstruction_cost = log_bernoulli(x, mux).sum(axis=1)\n",
    "#     #epsilon = 1e-8\n",
    "#     #reconstruction_cost =( x * T.log(mux + epsilon) + (1-x)*T.log(1-mux+epsilon))\n",
    "#     KL_qp = kl_normal_2_stdnormal(muq, lvq).sum(axis=1)#.mean()\n",
    "    \n",
    "#     LL = reconstruction_cost - KL_qp * KL_weigth\n",
    "    \n",
    "#     return LL, reconstruction_cost, KL_qp\n",
    "\n",
    "def M1_likelihood(x,x_rec,sigma_1,mu_1,KL_weigth=1):\n",
    "    \n",
    "    reconstruction_cost = log_bernoulli(x, x_rec).sum(axis=1)\n",
    "    \n",
    "    KL_1 = kl_normal_2_stdnormal(mu_1, sigma_1).sum(axis=1)\n",
    "    \n",
    "    LL = reconstruction_cost - KL_1 * KL_weigth\n",
    "    return LL, reconstruction_cost, KL_1\n",
    "\n",
    "def M2_likelihood(x,x_rec,sigma_1,mu_1, sigma_2,mu_2,KL_weigth=1):\n",
    "    \n",
    "    reconstruction_cost = log_bernoulli(x, x_rec).sum(axis=1)\n",
    "    \n",
    "    KL_1 = kl_normal_2_stdnormal(mu_1, sigma_1).sum(axis=1)\n",
    "    KL_2 = kl_normal_2_stdnormal(mu_2, sigma_2).sum(axis=1)\n",
    "\n",
    "    \n",
    "    LL = reconstruction_cost - (KL_1+KL_2) * KL_weigth\n",
    "    return LL, reconstruction_cost, KL_1,KL_2\n",
    "\n",
    "\n",
    "def LogSumExp(x, axis=None, keepdims=False):\n",
    "    ''' Numerically stable theano version of the Log-Sum-Exp trick'''\n",
    "    x_max = T.max(x, axis=axis, keepdims=True)\n",
    "\n",
    "    preres = T.log(T.sum(T.exp(x - x_max), axis=axis, keepdims=keepdims))\n",
    "    return preres + x_max.reshape(preres.shape)\n",
    "\n",
    "def img_transform(x,reverse=False):\n",
    "    if reverse:\n",
    "        x=x.swapaxes(1,2)\n",
    "        x=x.swapaxes(2,3)\n",
    "        x=x.reshape((x.shape[0],IMG_LEN*IMG_LEN*IMG_DEPTH))\n",
    "    else:\n",
    "        x=x.reshape((x.shape[0],IMG_LEN, IMG_LEN, IMG_DEPTH))\n",
    "        x=x.swapaxes(2,3)\n",
    "        x=x.swapaxes(1,2)\n",
    "    return x\n",
    "\n",
    "\n",
    "def module(l_in,module_type,out_size,hid_size=hid_size,skip_connections=skip_connections):\n",
    "#     if module_type=='classifier':\n",
    "#         l_in=L.DropoutLayer(l_in)\n",
    "    \n",
    "    l_1   = L.DenseLayer(l_in, \n",
    "                        num_units=hid_size,\n",
    "                        nonlinearity=rectify)#,\n",
    "    l_2   = L.DenseLayer(l_1,\n",
    "                            num_units=hid_size,\n",
    "                            nonlinearity=rectify)#,\n",
    "    if skip_connections:\n",
    "        l_2=L.ConcatLayer([l_1,l_2])\n",
    "    \n",
    "    if module_type=='latent_generator':\n",
    "        mu   = L.DenseLayer(l_2,\n",
    "                            num_units=out_size,\n",
    "                            nonlinearity=None)#,\n",
    "\n",
    "        sigma   = L.DenseLayer(l_2,\n",
    "                            num_units=out_size,\n",
    "                            nonlinearity=clip)#,\n",
    "        return mu,sigma\n",
    "    if module_type=='recon_generator':\n",
    "        l_out    = L.DenseLayer(l_2, \n",
    "                            num_units=out_size,\n",
    "                            nonlinearity=sigmoid)\n",
    "        return l_out\n",
    "    if module_type=='classifier':\n",
    "        \n",
    "        l_out    = L.DenseLayer(l_2, \n",
    "                            num_units=out_size,\n",
    "                            nonlinearity=softmax)\n",
    "        return l_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "### CREATE MODEL\n",
    "from lasagne.nonlinearities import leaky_rectify, rectify, sigmoid,softmax\n",
    "from parmesan.layers import SampleLayer\n",
    "### CLASSIFIER\n",
    "l_in_x   = L.InputLayer(shape=(None, num_features))#, name='l_in_x')\n",
    "l_in_y   = L.InputLayer(shape=(None, num_classes))#, name='l_in_y')\n",
    "l_in_z_1   = L.InputLayer(shape=(None, num_latent))#, name='l_in_y'\n",
    "l_in_z_2   = L.InputLayer(shape=(None, num_latent))#, name='l_in_y')\n",
    "\n",
    "### M1 ENCODER \n",
    "mu_1,sigma_1 = module(l_in_x,\n",
    "                     module_type='latent_generator',\n",
    "                     out_size=num_latent)\n",
    "l_z_1        = SampleLayer(mean=mu_1, \n",
    "                         log_var=sigma_1, \n",
    "                         eq_samples=eq_size, \n",
    "                         iw_samples=iw_size)#,\n",
    "### M1 DECODER\n",
    "x_rec_M1     = module(l_z_1,\n",
    "                     module_type='recon_generator',\n",
    "                     out_size=num_features)\n",
    "### Classifier\n",
    "y_out       = module(l_in_z_1,\n",
    "                     module_type='classifier',\n",
    "                     out_size=num_classes)\n",
    "\n",
    "## Entering of Y\n",
    "l_z_1_concat = L.ConcatLayer([l_in_z_1,l_in_y])\n",
    "\n",
    "mu_2,sigma_2 = module(l_z_1_concat,\n",
    "                     module_type='latent_generator',\n",
    "                     out_size=num_latent)\n",
    "\n",
    "l_z_2        = SampleLayer(mean=mu_2, \n",
    "                         log_var=sigma_2, \n",
    "                         eq_samples=eq_size, \n",
    "                         iw_samples=iw_size)#,\n",
    "\n",
    "### M2 DECODER\n",
    "l_z_2_concat = L.ConcatLayer([l_z_2,l_in_y])\n",
    "\n",
    "\n",
    "#### The part below is removed tempoarally in order to obtain faster convergence\n",
    "# mu_1_dec , sigma_1_dec = module(l_z_2_concat,\n",
    "#                      module_type='latent_generator',\n",
    "#                      out_size=num_latent)\n",
    "\n",
    "# l_z_1_dec    = SampleLayer(mean=mu_1_dec, \n",
    "#                          log_var=sigma_1_dec, \n",
    "#                          eq_samples=eq_size, \n",
    "#                          iw_samples=iw_size)#,\n",
    "\n",
    "# l_z_1_dec_con= L.ConcatLayer([l_z_1_dec,l_in_y])\n",
    "\n",
    "x_rec_M2     = module(l_z_2_concat,\n",
    "                     module_type='recon_generator',\n",
    "                     out_size=num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "from lasagne.objectives import categorical_crossentropy, categorical_accuracy\n",
    "\n",
    "##################GENERATING RECONSTRUCTIONS AND CLASSES\n",
    "\n",
    "sym_x = T.matrix('x') # (batch_size x 3072)\n",
    "sym_z = T.matrix('z') # Latent variable (batch_size x num_latent)\n",
    "sym_y = T.matrix('y_l') # Latent variable (batch_size x num_classes)\n",
    "sym_x_l = T.matrix('x_l') # Latent variable (batch_size x 3072)\n",
    "KL_weigth_sup = T.fscalar('KL_weigth_sup') \n",
    "KL_weigth_uns = T.fscalar('KL_weigth_uns')\n",
    "Alpha = T.fscalar('Alpha') \n",
    "Beta = T.fscalar('Beta')\n",
    "\n",
    "####### Repeatition of data for unsupervised learning\n",
    "\n",
    "py=T.dot(T.ones((sym_x.shape[0],1)),prior_y)\n",
    "\n",
    "\n",
    "#####################SUPERVISED###################\n",
    "mu_1_l, sigma_1_l, x_rec_M1_l,l_z_1_l = L.get_output([mu_1,sigma_1,x_rec_M1,l_z_1\n",
    "                        ],{l_in_x:x_train_l},deterministic = False) #Encoder\n",
    "y_train_l = L.get_output([y_out\n",
    "                        ],{l_in_z_1:l_z_1_l},deterministic = False)[0] #Encoder\n",
    "\n",
    "x_rec_M2_l, mu_2_l, sigma_2_l = L.get_output([x_rec_M2,mu_2,sigma_2],#DECODER\n",
    "                                           {l_in_z_1:l_z_1_l,l_in_y:t_train_l}, deterministic = False)\n",
    "\n",
    "\n",
    "#Likelihood\n",
    "    \n",
    "M2_like_l,rec_M2_l,KL_2_M1_l = M1_likelihood(x_train_l, x_rec_M2_l, sigma_2_l, mu_2_l, KL_weigth_sup)\n",
    "# M2_like_l,rec_M2_l,KL_1_M2_l,KL_2_M2_l = M2_likelihood(x_train_l, x_rec_M2_l,\n",
    "#                                                  sigma_1_l, mu_1_l,\n",
    "#                                                  sigma_2_l, mu_2_l,\n",
    "#                                                  KL_weigth_sup)\n",
    "\n",
    "\n",
    "smooth_y=(1-smooth_factor)*t_train_l + smooth_factor*T.dot(T.ones((t_train_l.shape[0],1)),prior_y) # Toke --> stochastic here\n",
    "\n",
    "LL_train_l=T.mean(M2_like_l+Alpha*T.sum(smooth_y*T.log(y_train_l+1e-8))) \n",
    "\n",
    "\n",
    "#############UNSUPERVISED####################\n",
    "mu_1_u, sigma_1_u, x_rec_M1_u,l_z_1_u = L.get_output([mu_1,sigma_1,x_rec_M1,l_z_1\n",
    "                        ],{l_in_x:sym_x},deterministic = False) #Encoder\n",
    "y_train_u = L.get_output([y_out\n",
    "                        ],{l_in_z_1:l_z_1_u},deterministic = False)[0] #Encoder\n",
    "    \n",
    "#Splitting on label\n",
    "t_eye = T.eye(num_classes, k=0)\n",
    "t_u = t_eye.reshape((num_classes, 1, num_classes)).repeat(sym_x.shape[0], axis=1).reshape((-1, num_classes))\n",
    "z_1_u = l_z_1_u.reshape((1, l_z_1_u.shape[0], l_z_1_u.shape[1])).repeat(num_classes, axis=0).reshape((-1, l_z_1_u.shape[1]))\n",
    "x_u = sym_x.reshape((1, sym_x.shape[0], sym_x.shape[1])).repeat(num_classes, axis=0).reshape((-1, sym_x.shape[1]))\n",
    "\n",
    "x_rec_M2_u, mu_2_u, sigma_2_u = L.get_output([x_rec_M2,mu_2,sigma_2],#DECODER\n",
    "                                           {l_in_z_1:z_1_u,l_in_y:t_u}, deterministic = False)\n",
    "\n",
    "#Replicate to fit dimentions of x_u\n",
    "mu_1_rep_u = mu_1_u.reshape((1, mu_1_u.shape[0], mu_1_u.shape[1])).repeat(num_classes, axis=0).reshape((-1, mu_1_u.shape[1]))\n",
    "sigma_1_rep_u = sigma_1_u.reshape((1, sigma_1_u.shape[0], sigma_1_u.shape[1])).repeat(num_classes, axis=0).reshape((-1, sigma_1_u.shape[1]))\n",
    "\n",
    "\n",
    "#Likelihood\n",
    "M1_like_u,rec_M1_u,KL_1_M1_u = M1_likelihood(sym_x, x_rec_M1_u, sigma_1_u, mu_1_u, KL_weigth_uns)\n",
    "M2_like_u,rec_M2_u,KL_2_M2_u = M1_likelihood(x_u, x_rec_M2_u, sigma_2_u, mu_2_u, KL_weigth_uns)\n",
    "\n",
    "# M2_like_u,rec_M2_u,KL_1_M2_u,KL_2_M2_u = M2_likelihood(x_u, x_rec_M2_u,\n",
    "#                                                  sigma_1_rep_u, mu_1_rep_u,\n",
    "#                                                  sigma_2_u, mu_2_u,\n",
    "#                                                  KL_weigth_uns)\n",
    "#M2_likelihood\n",
    "M2_like_u = M2_like_u.reshape((num_classes,sym_x.shape[0])).T\n",
    "M2_like_u+=-T.log(y_train_u+1e-8)*Beta+T.log(py)\n",
    "LL_train_u=T.mean(T.sum(y_train_u*M2_like_u,axis=1))\n",
    "\n",
    "\n",
    "################## EVALUATION##############################\n",
    "mu_1_eval, sigma_1_eval, x_rec_M1_eval,l_z_1_eval = L.get_output([mu_1,sigma_1,x_rec_M1,l_z_1\n",
    "                        ],{l_in_x:sym_x},deterministic = True) #Encoder\n",
    "y_train_eval = L.get_output([y_out\n",
    "                        ],{l_in_z_1:l_z_1_eval},deterministic = True)[0] #Encoder\n",
    "    \n",
    "#Splitting on label\n",
    "t_eye = T.eye(num_classes, k=0)\n",
    "t_eval = t_eye.reshape((num_classes, 1, num_classes)).repeat(sym_x.shape[0], axis=1).reshape((-1, num_classes))\n",
    "z_1_eval = l_z_1_eval.reshape((1, l_z_1_eval.shape[0], l_z_1_eval.shape[1])).repeat(num_classes, axis=0).reshape((-1, l_z_1_eval.shape[1]))\n",
    "x_eval = sym_x.reshape((1, sym_x.shape[0], sym_x.shape[1])).repeat(num_classes, axis=0).reshape((-1, sym_x.shape[1]))\n",
    "\n",
    "x_rec_M2_eval, mu_2_eval, sigma_2_eval = L.get_output([x_rec_M2,mu_2,sigma_2],#DECODER\n",
    "                                           {l_in_z_1:z_1_eval,l_in_y:t_eval}, deterministic = True)\n",
    "\n",
    "\n",
    "\n",
    "#Likelihood\n",
    "\n",
    "M1_like_eval,rec_M1_eval,KL_1_M1_eval = M1_likelihood(sym_x, x_rec_M1_eval, sigma_1_eval, mu_1_eval, KL_weigth_uns)\n",
    "\n",
    "#Replicate to use for M2\n",
    "mu_1_rep_eval = mu_1_eval.reshape((1, mu_1_eval.shape[0], mu_1_eval.shape[1])).repeat(num_classes, axis=0).reshape((-1, mu_1_eval.shape[1]))\n",
    "sigma_1_rep_eval = sigma_1_eval.reshape((1, sigma_1_eval.shape[0], sigma_1_eval.shape[1])).repeat(num_classes, axis=0).reshape((-1, sigma_1_eval.shape[1]))\n",
    "\n",
    "\n",
    "M2_like_eval,rec_M2_eval,KL_1_M2_eval = M1_likelihood(x_eval, x_rec_M2_eval, sigma_2_eval, mu_2_eval, KL_weigth_uns)\n",
    "\n",
    "# M2_like_eval,rec_M2_eval,KL_1_M2_eval,KL_2_M2_eval = M2_likelihood(x_eval, x_rec_M2_eval,\n",
    "#                                                  sigma_1_rep_eval, mu_1_rep_eval,\n",
    "#                                                  sigma_2_eval, mu_2_eval,\n",
    "#                                                  KL_weigth_uns)\n",
    "#M2_likelihood\n",
    "M2_like_eval = M2_like_eval.reshape((num_classes,sym_x.shape[0])).T\n",
    "M2_like_eval+=-T.log(y_train_eval+1e-8)*Beta+T.log(py)\n",
    "LL_train_eval=T.mean(T.sum(y_train_eval*M2_like_eval,axis=1))\n",
    "\n",
    "\n",
    "\n",
    "### Combining likelihood\n",
    "LL=LL_train_u+LL_train_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# f_temp=theano.function([sym_x, In(KL_weigth_uns, value=KL_weigth_uns_in)\n",
    "#                                 ],[M1_like_eval])\n",
    "# out=f_temp(x_train_l)\n",
    "# print(out[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define variables to output\n",
    "#LL_train,LL_train_u,LL_train_l allready defined\n",
    "certainty_class_eval=T.mean(T.max(y_train_eval,axis=-1))\n",
    "prob_recon_eval=T.exp(M2_like_eval-LogSumExp(M2_like_eval,axis=-1,keepdims=True))\n",
    "certainty_recon_eval=T.mean(T.max(prob_recon_eval,axis=-1))\n",
    "# mean_var_eval=T.mean(lv_train_eval)\n",
    "recon_class_same_eval=categorical_accuracy(prob_recon_eval,y_train_eval).mean()\n",
    "\n",
    "\n",
    "#Evaluation output\n",
    "# prob_recon_eval=T.exp(LL_rec_eval-LogSumExp(LL_rec_eval,axis=-1,keepdims=True))\n",
    "\n",
    "#create KL divergence mean unwiegthed - IMPROVE\n",
    "# KL_train_u = KL_train_u.reshape((num_classes,sym_x.shape[0])).T\n",
    "# KL_train_l = KL_train_l.reshape((num_classes,sym_x_l.shape[0])).T\n",
    "# mean_KL_train_u=T.sum(y_train_u*KL_train_u,axis=1).mean()\n",
    "# mean_KL_train_l=T.sum(sym_y*KL_train_l,axis=1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from theano import In\n",
    "### M1 train\n",
    "M1_params = L.get_all_params([x_rec_M1], trainable=True)\n",
    "M1_grads  = T.grad(-M1_like_u.mean(), M1_params)\n",
    "M1_updates    = lasagne.updates.adam(M1_grads, M1_params,\n",
    "                                  learning_rate=learning_rate)\n",
    "\n",
    "f_train_M1 = theano.function(inputs=[sym_x,\n",
    "                                 In(KL_weigth_uns, value=KL_weigth_uns_in),\n",
    "                                 ],\n",
    "                          outputs=[M1_like_u,x_rec_M1_u],\n",
    "                          updates=M1_updates)\n",
    "f_eval_M1 = theano.function(inputs=[sym_x,\n",
    "                                 In(KL_weigth_uns, value=KL_weigth_uns_in),\n",
    "                                 ],\n",
    "                          outputs=[M1_like_eval.mean(),KL_1_M1_eval.mean()])\n",
    "\n",
    "f_reconstruction_M1 = theano.function(inputs=[sym_x], outputs=[x_rec_M1_eval,l_z_1_eval])\n",
    "\n",
    "### M2 train\n",
    "M2_params = L.get_all_params([y_out, x_rec_M2], trainable=True)\n",
    "M2_grads  = T.grad(-LL, M2_params)\n",
    "M2_updates    = lasagne.updates.adam(M2_grads, M2_params,\n",
    "                                  learning_rate=learning_rate)\n",
    "\n",
    "f_train_M2 = theano.function(inputs=[sym_x,\n",
    "                                In(KL_weigth_uns, value=KL_weigth_uns_in),\n",
    "                            In(KL_weigth_sup, value=KL_weigth_sup_in),\n",
    "                        In(Beta, value=Beta_in),In(Alpha, value=Alpha_in)],\n",
    "                          outputs=[LL,M2_like_u,M2_like_l],\n",
    "                          updates=M2_updates)\n",
    "\n",
    "f_eval_M2 = theano.function(inputs=[sym_x,\n",
    "                                  In(KL_weigth_uns, value=KL_weigth_uns_in),\n",
    "                                  In(Beta, value=Beta_in)],outputs=[y_train_eval,M2_like_eval\n",
    "                                                                    ,certainty_class_eval,certainty_recon_eval,recon_class_same_eval\n",
    "                                                                   ,KL_1_M1_eval.mean(),KL_1_M2_eval.mean()])\n",
    "\n",
    "f_reconstruction_M2 = theano.function(inputs=[sym_x], outputs=[x_rec_M2_eval,l_z_1_eval,mu_2_eval])\n",
    "\n",
    "\n",
    "# ### M2+M1 unsupervised train\n",
    "# M2_params_uns = L.get_all_params([l_z_1,y_out, x_rec_M2], trainable=True)\n",
    "# M2_grads_uns  = T.grad(-LL_train_u, M2_params_uns)\n",
    "# M2_updates_uns    = lasagne.updates.adam(M2_grads_uns, M2_params_uns,\n",
    "#                                   learning_rate=learning_rate)\n",
    "\n",
    "# f_train_M2_uns = theano.function(inputs=[sym_x,\n",
    "#                                 In(KL_weigth_uns, value=KL_weigth_uns_in),\n",
    "#                         In(Beta, value=Beta_in)],\n",
    "#                           outputs=[],\n",
    "#                           updates=M2_updates_uns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# KL_w_uns=1\n",
    "# KL_w_sup=2\n",
    "# from theano import In\n",
    "# KL_weigth_sup = T.dscalar('KL_weigth_sup') # Weigth \n",
    "# KL_weigth_uns = T.dscalar('KL_weigth_uns') # Latent variable (batch_size x 3072)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#print(f_eval_M2(x_train_l)[0].shape)\n",
    "#print(f_reconstruction_M1(x_train_l)[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def augment_image(x,eps=1e-3, noise_sigma=0.1):\n",
    "    x_trans=np.log((x+eps)/(1-x+eps))\n",
    "    x_trans=x_trans.reshape((x.shape[0],IMG_LEN,IMG_LEN,IMG_DEPTH))\n",
    "    for i in range(IMG_DEPTH):\n",
    "        x_trans[:,:,:,i]+=np.random.normal(0,noise_sigma)\n",
    "    x_trans=(x_trans+np.random.normal(0,noise_sigma))*np.exp(np.random.normal(0,noise_sigma*4))+np.random.normal(0,noise_sigma)\n",
    "    \n",
    "    x_trans+=np.random.normal(0,noise_sigma, (x_trans.shape)).astype('float32')\n",
    "    \n",
    "    x_out=(np.exp(x_trans)/(1+np.exp(x_trans))).reshape((x.shape[0],IMG_LEN*IMG_LEN*IMG_DEPTH))\n",
    "    x_out=np.nan_to_num(x_out) \n",
    "    x_out=np.clip(x_out,0,1)\n",
    "\n",
    "    return x_out\n",
    "def pretty_print(title,output_dict,var_list,var_list_extra=[],show=True):\n",
    "    color_list=[\"blue\",\"red\",\"green\",\"cyan\",\"magenta\",\"yellow\",\"black\"]\n",
    "    plt.xlabel('Updates')\n",
    "    for i in range(len(var_list)):\n",
    "        plt.plot(output_dict['samples_processed'], output_dict[var_list[i]], \n",
    "                 color=color_list[i], label=var_list[i])\n",
    "        if i<len(var_list_extra):\n",
    "            plt.plot(output_dict['samples_processed'], output_dict[var_list_extra[i]], \n",
    "                 color=color_list[i], label=var_list_extra[i],linestyle=\"--\")\n",
    "    \n",
    "    plt.legend(loc=2)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "    plt.grid('on')\n",
    "    plt.tight_layout()\n",
    "    plt.ylim(0, 1)\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Training')\n",
    "batch_size=1000\n",
    "# #M1 pretraining\n",
    "for i in range(5000):\n",
    "    idxs = np.random.choice(range(x_trai.shape[0]), size=(batch_size), replace=False) \n",
    "    x_batch = x_trai[idxs]\n",
    "    f_train_M1(x_batch)\n",
    "    if i%100==0:\n",
    "        out=f_eval_M1(x_test)\n",
    "        print(out[0].tolist(),out[1].tolist())\n",
    "# #Combined training\n",
    "print('Pre training finished')\n",
    "print('acc,     LL, cert_cl,cert_rec,rec_cl_same,KL_M1,KL_M2')\n",
    "for i in range(10000):\n",
    "    idxs = np.random.choice(range(x_trai.shape[0]), size=(batch_size), replace=False) \n",
    "    x_batch = x_trai[idxs]\n",
    "    t_batch = t_trai[idxs]\n",
    "    f_train_M2(x_batch)#,x_train_l,t_train_l)\n",
    "    if i%100==0:\n",
    "        out=f_eval_M2(x_test)\n",
    "        print(round(np.sum(np.equal(np.argmax(out[0],axis=1),\n",
    "            np.argmax(t_test,axis=1)))/float(t_test.shape[0]),4),round(out[1].mean(),2)\n",
    "             ,round(out[2].tolist(),4),round(out[3].tolist(),4),round(out[4].tolist(),4),round(out[5].tolist(),2),round(out[6].tolist(),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#temp=L.get_all_layers([y_out])\n",
    "temp=L.get_all_param_values([y_out,x_rec_M2])\n",
    "for i in range(len(temp)):\n",
    "    print(i,temp[i].shape)\n",
    "#print(L.get_all_params([x_rec_M1], trainable=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "from IPython.display import Image, display, clear_output\n",
    "import sklearn.decomposition\n",
    "pca = sklearn.decomposition.PCA(n_components=2)\n",
    "\n",
    "output_dict={'samples_processed':[]\n",
    "             ,'LL_train':[],\n",
    "             'LL_train_u':[],\n",
    "             'LL_train_l':[],\n",
    "            'LL_valid':[],\n",
    "            'certainty_class_u':[],\n",
    "            'certainty_recon_u':[],\n",
    "            'certainty_class_l':[],\n",
    "            'recon_class_same_u':[],\n",
    "            'mean_var_u':[],\n",
    "            'mean_var_l':[],\n",
    "            'KL_train_u':[],\n",
    "            'KL_train_l':[],\n",
    "            'acc_trai_rec':[],\n",
    "            'acc_trai_class':[],\n",
    "            'trai_class_rec_same':[],            \n",
    "            'acc_vali_rec':[],\n",
    "            'acc_vali_class':[],\n",
    "            'vali_class_rec_same':[]}\n",
    "\n",
    "LL_train, KL_train, logpx_train = [],[],[]\n",
    "LL_valid, KL_valid, logpx_valid = [],[],[]\n",
    "samples_processed = 0\n",
    "plt_vals = []\n",
    "plt_vals_t = []\n",
    "samples_processed =0\n",
    "valid_samples_processed = []\n",
    "scalarInputs=11\n",
    "\n",
    "out_accum=[0]*(scalarInputs)\n",
    "\n",
    "idxs_ran = np.random.choice(range(x_trai.shape[0]), size=(x_trai.shape[0]), replace=False) \n",
    "train_pred_class=t_trai[idxs_ran]\n",
    "idxs_ran = np.random.choice(range(x_trai.shape[0]), size=(x_trai.shape[0]), replace=False) \n",
    "train_pred_recon=t_trai[idxs_ran]\n",
    "\n",
    "\n",
    "num_validation=0\n",
    "try:\n",
    "    while samples_processed < samples_to_process:\n",
    "#        print(\"Number of samples processed: {}\".format(samples_processed))\n",
    "        idxs = np.random.choice(range(x_trai.shape[0]), size=(batch_size), replace=False) \n",
    "        x_batch = x_trai[idxs]\n",
    "        t_batch = t_trai[idxs]\n",
    "\n",
    "        if augmented_train:\n",
    "            #x_batch_ber=np.random.binomial(1,x_batch,size=x_batch.shape).astype(theano.config.floatX)\n",
    "            #x_train_l_ber=np.random.binomial(1,x_train_l,size=x_train_l.shape).astype(theano.config.floatX)\n",
    "            x_batch_ber=augment_image(x_batch)\n",
    "            x_train_l_ber=augment_image(x_train_l,noise_sigma=0.2)\n",
    "            if np.isnan(x_batch_ber).any():\n",
    "                print('x_trai is nan')\n",
    "                break\n",
    "            if np.isnan(x_train_l_ber).any():\n",
    "                print('x_trai_l is nan')\n",
    "                break\n",
    "            out = f_train(x_batch_ber, x_train_l_ber, t_train_l)\n",
    "        else:\n",
    "            out = f_train(x_batch, x_train_l, t_train_l)\n",
    "#        uns_results.add_batch([out,t_batch],batch_size)\n",
    "        train_pred_recon[idxs]=out[scalarInputs]\n",
    "        train_pred_class[idxs]=out[scalarInputs+1]\n",
    "        \n",
    "        mean_factor=min(0.99,samples_processed/batch_size)\n",
    "        samples_processed += batch_size\n",
    "\n",
    "#        print(out_accum[i],mean_factor,(1-mean_factor),out[i])\n",
    "\n",
    "        for i in range(scalarInputs):\n",
    "            out_accum[i]=out_accum[i]*mean_factor+(1-mean_factor)*out[i].mean()\n",
    "        \n",
    "\n",
    "        \n",
    "        if samples_processed >= (num_validation+1)*val_interval:\n",
    "            valid_samples_processed += [samples_processed]\n",
    "            #size_f(y_eval,LL_rec_eval)\n",
    "\n",
    "            z_eval = f_z(x_vali)[0]\n",
    "            \n",
    "            out = f_eval_recon(x_vali)\n",
    "            LL_valid += [out[0]]\n",
    "            pred_y_class=out[1]\n",
    "            pred_y_recon=out[2]\n",
    "            \n",
    "\n",
    "            \n",
    "            #Confusion matrix validation\n",
    "            pred_y_class_vec=np.argmax(pred_y_class,axis=1)\n",
    "            pred_y_recon_vec=np.argmax(pred_y_recon,axis=1)\n",
    "            t_vali_vec=np.argmax(t_vali,axis=1)\n",
    "            pred_y_class_vec_correct=np.equal(pred_y_class_vec,t_vali_vec).astype('int')\n",
    "            pred_y_recon_vec_correct=np.equal(pred_y_recon_vec,t_vali_vec).astype('int')\n",
    "            confussion_class=np.zeros((num_classes,num_classes))\n",
    "            confussion_recon=np.zeros((num_classes,num_classes))\n",
    "            confussion_class_recon=np.zeros((2,2))\n",
    "            for i in range(t_vali.shape[0]):\n",
    "                confussion_class[pred_y_class_vec[i],t_vali_vec[i]]+=1\n",
    "                confussion_recon[pred_y_recon_vec[i],t_vali_vec[i]]+=1\n",
    "                confussion_class_recon[pred_y_recon_vec_correct[i],pred_y_class_vec_correct[i]]+=1\n",
    "            confussion_class/=(t_vali.shape[0]*1.0)\n",
    "            confussion_recon/=(t_vali.shape[0]*1.0)\n",
    "            confussion_class_recon/=(t_vali.shape[0]*1.0)\n",
    "            \n",
    "            ##Accuracies use confusion matrix istead\n",
    "            acc_vali_rec=np.sum(np.equal(np.argmax(pred_y_recon,axis=1),np.argmax(t_vali,axis=1)))/(t_vali.shape[0]*1.0)\n",
    "            acc_vali_class=np.sum(np.equal(np.argmax(pred_y_class,axis=1),np.argmax(t_vali,axis=1)))/(t_vali.shape[0]*1.0)\n",
    "            vali_class_rec_same=np.sum(np.equal(np.argmax(pred_y_recon,axis=1),np.argmax(pred_y_class,axis=1)))/(t_vali.shape[0]*1.0)\n",
    "            \n",
    "            acc_trai_rec=np.sum(np.equal(np.argmax(train_pred_recon,axis=1),np.argmax(t_trai,axis=1)))/(t_trai.shape[0]*1.0)\n",
    "            acc_trai_class=np.sum(np.equal(np.argmax(train_pred_class,axis=1),np.argmax(t_trai,axis=1)))/(t_trai.shape[0]*1.0)\n",
    "            trai_class_rec_same=np.sum(np.equal(np.argmax(train_pred_class,axis=1),np.argmax(train_pred_recon,axis=1)))/(t_trai.shape[0]*1.0)\n",
    "            \n",
    "            ####Update Output dict\n",
    "            output_dict['samples_processed']+=[samples_processed]\n",
    "            output_dict['LL_train']+=[out_accum[0]]\n",
    "            output_dict['LL_train_u']+=[out_accum[1]]\n",
    "            output_dict['LL_train_l']+=[out_accum[2]]\n",
    "            output_dict['LL_valid']+=[np.mean(out[0])]\n",
    "            output_dict['certainty_class_u']+=[out_accum[3]]\n",
    "            output_dict['certainty_recon_u']+=[out_accum[6]]\n",
    "            output_dict['certainty_class_l']+=[out_accum[7]]\n",
    "            output_dict['recon_class_same_u']+=[out_accum[5]]\n",
    "            output_dict['mean_var_u']+=[out_accum[4]]\n",
    "            output_dict['mean_var_l']+=[out_accum[8]]\n",
    "            output_dict['KL_train_u']+=[out_accum[9]]\n",
    "            output_dict['KL_train_l']+=[out_accum[10]]\n",
    "            output_dict['acc_trai_rec']+=[acc_trai_rec]\n",
    "            output_dict['acc_trai_class']+=[acc_trai_class]\n",
    "            output_dict['trai_class_rec_same']+=[trai_class_rec_same]            \n",
    "            output_dict['acc_vali_rec']+=[acc_vali_rec]\n",
    "            output_dict['acc_vali_class']+=[acc_vali_class]\n",
    "            output_dict['vali_class_rec_same']+=[vali_class_rec_same]\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            ####Printing outputs\n",
    "            for i in sorted(output_dict.keys()):\n",
    "                print(i,output_dict[i][-1])\n",
    "            \n",
    "            ##Printing confusion matrixes\n",
    "            print('confussion matrix classifyer validation data')\n",
    "            print(confussion_class)\n",
    "            print('confussion matrix reconstruction validation data')\n",
    "            print(confussion_recon)\n",
    "            print('Mutual confusion matrix class and recon being correct - validation data')\n",
    "            print(confussion_class_recon)\n",
    "            \n",
    "            ###############\n",
    "            \n",
    "            ### Plot learning curves \n",
    "            pretty_print(\"Training and validation accuracies\",output_dict,['acc_trai_class','acc_trai_rec'],['acc_vali_class','acc_vali_rec'])\n",
    "\n",
    "            plt.subplot(2,2,1)\n",
    "            pretty_print(\"KL_divergence\",output_dict,['KL_train_u','KL_train_l'],show=False)\n",
    "\n",
    "\n",
    "\n",
    "            ### Plot PCA of latent space\n",
    "            plt.subplot(2,2,2)\n",
    "            plt.cla()\n",
    "            plt.xlabel('PCA 0'), plt.ylabel('PCA 1')\n",
    "            color = iter(plt.get_cmap('brg')(np.linspace(0, 1.0, num_classes)))\n",
    "            for i in range(num_classes):\n",
    "                clr = next(color)\n",
    "                pca_trans = pca.fit_transform(z_eval)\n",
    "                plt.scatter(pca_trans[tls.onehot2int(t_vali)==i, 0], \n",
    "                            pca_trans[tls.onehot2int(t_vali)==i, 1], \n",
    "                            c=clr, \n",
    "                            s=5., \n",
    "                            lw=0, \n",
    "                            marker='o', )\n",
    "            plt.grid('on')\n",
    "            plt.show()\n",
    "\n",
    "            \n",
    "            \n",
    "            t_plots=4\n",
    "            x_plot=x_vali[np.random.choice(range(x_vali.shape[0]), size=(t_plots*t_plots), replace=False)]\n",
    "            x_plot_recon = f_reconstruction(x_plot)\n",
    "            org_plot=tls.plot_svhn(x_plot, \n",
    "                                             t=t_plots, cmap=cmap, IMG_LEN=IMG_LEN, \n",
    "                                             IMG_DEPTH=IMG_DEPTH,choose_random=False)\n",
    "            #print(type(org_plot),org_plot)\n",
    "            _, ax = plt.subplots(1,num_classes+1)\n",
    "            ax[0].imshow(org_plot[0], cmap=cmap, interpolation='None')\n",
    "            ax[0].set_title('Original')\n",
    "            ax[0].axis('off')\n",
    "\n",
    "            \n",
    "            for c in range(num_classes):\n",
    "                rec_plot=tls.plot_svhn(x_plot_recon[0][np.array(range(t_plots*t_plots))+(t_plots*t_plots)*c,:],t=t_plots, cmap=cmap, IMG_LEN=IMG_LEN, \n",
    "                                             IMG_DEPTH=IMG_DEPTH,choose_random=False)\n",
    "                ax[c+1].imshow(rec_plot[0], cmap=cmap, interpolation='None')\n",
    "                ax[c+1].set_title('Reconstruction')\n",
    "                ax[c+1].axis('off')\n",
    "\n",
    "                #plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            x_plot=x_train_l[np.random.choice(range(x_train_l.shape[0]), size=(t_plots*t_plots), replace=False)]\n",
    "            x_plot_recon = f_reconstruction(x_plot)\n",
    "            org_plot=tls.plot_svhn(x_plot, \n",
    "                                             t=t_plots, cmap=cmap, IMG_LEN=IMG_LEN, \n",
    "                                             IMG_DEPTH=IMG_DEPTH,choose_random=False)\n",
    "            #print(type(org_plot),org_plot)\n",
    "            _, ax = plt.subplots(1,num_classes+1)\n",
    "            ax[0].imshow(org_plot[0], cmap=cmap, interpolation='None')\n",
    "            ax[0].set_title('Original - labeled')\n",
    "            ax[0].axis('off')\n",
    "            \n",
    "            for c in range(num_classes):\n",
    "                rec_plot=tls.plot_svhn(x_plot_recon[0][np.array(range(t_plots*t_plots))+(t_plots*t_plots)*c,:],\n",
    "                                       t=t_plots, cmap=cmap, IMG_LEN=IMG_LEN, \n",
    "                                       IMG_DEPTH=IMG_DEPTH, choose_random=False)\n",
    "                ax[c+1].imshow(rec_plot[0], cmap=cmap, interpolation='None')\n",
    "                #ax[c+1].set_title('Reconstruction- labeled')\n",
    "                ax[c+1].axis('off')\n",
    "\n",
    "#             plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "            x_plot=x_trai[np.random.choice(range(x_trai.shape[0]), size=(num_classes+1), replace=False)]\n",
    "            x_plot_recon = f_reconstruction(x_plot)[0]\n",
    "            x_plot_comb=np.zeros(((num_classes+1)*(num_classes+2),num_features))\n",
    "            for i in range(num_classes+1):\n",
    "                x_plot_comb[i*(num_classes+1)]=x_plot[i]\n",
    "                #x_plot_comb[range(i*(num_classes+1)+1,(i+1)*(num_classes+1))]=x_plot_recon[range(i*num_classes,(i+1)*num_classes)]\n",
    "                x_plot_comb[range(i*(num_classes+1)+1,(i+1)*(num_classes+1))]=x_plot_recon[np.array(range(num_classes))*(num_classes+1)+i]\n",
    "            org_plot=tls.plot_svhn(x_plot_comb, \n",
    "                                             t=num_classes+1, cmap=cmap, IMG_LEN=IMG_LEN, \n",
    "                                             IMG_DEPTH=IMG_DEPTH,choose_random=False)\n",
    "            #print(type(org_plot),org_plot)\n",
    "            _, ax = plt.subplots(1,2)\n",
    "            ax[0].imshow(org_plot[0], cmap=cmap, interpolation='None')\n",
    "            ax[0].set_title('Original - labeled')\n",
    "            ax[0].axis('off')\n",
    "\n",
    "\n",
    "            plt.show()\n",
    "            num_validation+=1\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ! conda install nomkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "if False:\n",
    "    TSNEModel=TSNE(n_components=2, random_state=0);\n",
    "    TSNEFIT=TSNEModel.fit_transform(z_eval)\n",
    "    plt.cla()\n",
    "    plt.title('Latent space')\n",
    "    plt.xlabel('z0'), plt.ylabel('z1')\n",
    "    plt.xlabel('z0'), plt.ylabel('z1')\n",
    "    color = iter(plt.get_cmap('brg')(np.linspace(0, 1.0, num_classes)))\n",
    "    for i in range(num_classes):\n",
    "        clr = next(color)\n",
    "        plt.scatter(TSNEFIT[t_vali==i,0],TSNEFIT[t_vali==i,1], c=clr, s=5., lw=0, marker='o', )\n",
    "    plt.grid('on')\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Visualization of data augmentation\n",
    "\n",
    "num_image=6\n",
    "_, ax = plt.subplots(1,2)\n",
    "t_plots=4\n",
    "org_plot=tls.plot_svhn(x_trai[25:50], \n",
    "                                 t=t_plots, cmap=cmap, IMG_LEN=IMG_LEN, \n",
    "                                 IMG_DEPTH=IMG_DEPTH,choose_random=False)\n",
    "#print(type(org_plot),org_plot)\n",
    "\n",
    "ax[0].imshow(org_plot[0], cmap=cmap, interpolation='None')\n",
    "ax[0].set_title('Original - labeled')\n",
    "ax[0].axis('off')\n",
    "x_plot=x_trai[25+i]*0\n",
    "for i in range(1,2):\n",
    "    for i in range(25):\n",
    "        x_plot[i]=augment_image(x_trai[25+i].reshape((1,num_features)),noise_sigma=0.2)\n",
    "\n",
    "    t_plots=4\n",
    "    org_plot=tls.plot_svhn(x_plot, \n",
    "                                     t=t_plots, cmap=cmap, IMG_LEN=IMG_LEN, \n",
    "                                     IMG_DEPTH=IMG_DEPTH,choose_random=False)\n",
    "    #print(type(org_plot),org_plot)\n",
    "    \n",
    "    ax[i].imshow(org_plot[0], cmap=cmap, interpolation='None')\n",
    "    ax[i].set_title('Original - labeled')\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = f_train(x_batch, x_train_l, t_train_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "augment_image(x_trai[25+i].reshape((1,num_features)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_plot=x_trai[np.random.choice(range(x_trai.shape[0]), size=(num_classes+1), replace=False)]\n",
    "x_plot_recon = f_reconstruction(x_plot)[0]\n",
    "print('1')\n",
    "x_plot_comb=np.zeros(((num_classes+1)*(num_classes+2),num_features))\n",
    "for i in range(num_classes+1):\n",
    "    x_plot_comb[i*(num_classes+1)]=x_plot[i]\n",
    "    #x_plot_comb[range(i*(num_classes+1)+1,(i+1)*(num_classes+1))]=x_plot_recon[range(i*num_classes,(i+1)*num_classes)]\n",
    "    x_plot_comb[range(i*(num_classes+1)+1,(i+1)*(num_classes+1))]=x_plot_recon[np.array(range(num_classes))*(num_classes+1)+i]\n",
    "org_plot=tls.plot_svhn(x_plot_comb, \n",
    "                                 t=num_classes+1, cmap=cmap, IMG_LEN=IMG_LEN, \n",
    "                                 IMG_DEPTH=IMG_DEPTH,choose_random=False)\n",
    "#print(type(org_plot),org_plot)\n",
    "_, ax = plt.subplots(1,2)\n",
    "ax[0].imshow(org_plot[0], cmap=cmap, interpolation='None')\n",
    "ax[0].set_title('Original - labeled')\n",
    "ax[0].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tiljet: L2, image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LL_train_u=-2000\n",
    "LL_train_l=-1900\n",
    "max_like=np.max([-LL_train_u,-LL_train_l])\n",
    "LL_train=-(max_like+np.log(1/2*np.exp(-LL_train_l-max_like)+1/2*np.exp(-LL_train_u-max_like)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LL_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_like=np.max([-LL_train_u,-LL_train_l])\n",
    "print(max_like,-LL_train_l-max_like,-LL_train_u-max_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

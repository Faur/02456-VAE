{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Python 2.7\n",
    "\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline \n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "print(sys.version)\n",
    "\n",
    "import os\n",
    "import cPickle \n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne \n",
    "import lasagne.layers as L\n",
    "import parmesan\n",
    "import cPickle as pickle\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tools as tls\n",
    "from data_loaders import svhn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### GLOBAL PARAMETERS ###\n",
    "plot_train   = True\n",
    "using_shared = False\n",
    "\n",
    "### META - HOW THE PROGRAM WORKS\n",
    "\n",
    "np.random.seed(1234) # reproducibility\n",
    "\n",
    "\n",
    "### CONSTANTS\n",
    "dataset = 'MNIST'\n",
    "dataset = 'SVHN'\n",
    "print('dataset = {}'.format(dataset))\n",
    "\n",
    "if dataset == 'SVHN':    \n",
    "    file_name = 'data_no_share_c3' # assumes '.pkl'\n",
    "    IMG_LEN = 32\n",
    "    IMG_DEPTH = 3\n",
    "    cmap = None\n",
    "elif dataset == 'MNIST':\n",
    "    IMG_LEN = 28\n",
    "    IMG_DEPTH = 1\n",
    "    cmap = 'gray'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### PLOT SETTINGS\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "font_size = 15\n",
    "plt.rc('font',   size=font_size)       # controls default text sizes\n",
    "plt.rc('axes',   titlesize=font_size)  # fontsize of the axes title\n",
    "plt.rc('axes',   labelsize=font_size)  # fontsize of the x any y labels\n",
    "plt.rc('xtick',  labelsize=font_size)  # fontsize of the tick labels\n",
    "plt.rc('ytick',  labelsize=font_size)  # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=font_size)   # legend fontsize\n",
    "plt.rc('figure', titlesize=font_size)  # # size of the figure title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "### LOAD DATA\n",
    "\n",
    "if dataset == 'SVHN':\n",
    "    full_path = os.path.join(os.getcwd(), 'data')\n",
    "    full_path = os.path.join(full_path, file_name)\n",
    "    full_path  += '.pkl'\n",
    "    print(full_path)\n",
    "\n",
    "    with open(full_path, 'rb') as f:\n",
    "        x_trai, t_trai, x_vali, t_vali, x_test, t_test = pickle.load(f)\n",
    "\n",
    "    x_trai = x_trai/255\n",
    "    x_vali = x_vali/255\n",
    "    x_test = x_test/255\n",
    "\n",
    "\n",
    "elif dataset == 'MNIST':\n",
    "    full_path = os.path.join(os.getcwd(), 'data')\n",
    "    full_path = os.path.join(full_path  , 'mnist.npz')\n",
    "\n",
    "    data = np.load(full_path)\n",
    "    x_trai = data['X_train'].astype('float32')\n",
    "    t_trai = data['y_train'].astype('int32')\n",
    "\n",
    "    x_vali = data['X_valid'].astype('float32')\n",
    "    t_vali = data['y_valid'].astype('int32')\n",
    "\n",
    "    x_test = data['X_test'].astype('float32')\n",
    "    t_test = data['y_test'].astype('int32')\n",
    "\n",
    "\n",
    "    t_trai = tls.onehot(t_trai, 10)\n",
    "    t_vali = tls.onehot(t_vali, 10)\n",
    "    t_test = tls.onehot(t_test, 10)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "print('Size of total dataset: {:.2f} MB'.format(\n",
    "        (\n",
    "              sys.getsizeof(x_trai)\n",
    "            + sys.getsizeof(t_trai)\n",
    "            + sys.getsizeof(x_vali)\n",
    "            + sys.getsizeof(t_vali)\n",
    "            + sys.getsizeof(x_test)\n",
    "            + sys.getsizeof(t_test)\n",
    "        )/1.0e6))\n",
    "\n",
    "num_classes = t_trai.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a subset of labeled data points\n",
    "\n",
    "num_labeled = 100 # You decide on the size of the fraction...\n",
    "\n",
    "idxs_train_l = []\n",
    "for i in range(num_classes):\n",
    "    idxs = np.where(np.argmax(t_trai,axis=-1) == i)[0]\n",
    "    idxs_train_l += np.random.choice(idxs, size=num_labeled).tolist()\n",
    "\n",
    "x_train_l = x_trai[idxs_train_l].astype('float32')\n",
    "t_train_l = (t_trai[idxs_train_l,:]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### CHECK DATA\n",
    "print('Number of classes {}'.format(num_classes))\n",
    "\n",
    "num_features = x_trai[0].shape[0]\n",
    "print('Number of features {}'.format(num_features))\n",
    "\n",
    "print('')\n",
    "print('Train shape: ', \n",
    "      x_trai.shape, t_trai.shape)\n",
    "\n",
    "print('Valid shape: ', \n",
    "      x_vali.shape, t_vali.shape)\n",
    "\n",
    "print('Test shape:  ', \n",
    "      x_test.shape, t_test.shape)\n",
    "\n",
    "print('{}'.format(type(x_trai)))\n",
    "print('{}'.format(type(x_vali)))\n",
    "print('{}'.format(type(x_test)))\n",
    "print('')\n",
    "\n",
    "print('Prior')\n",
    "print(np.sum(t_trai, axis=0)/t_trai.shape[0])\n",
    "print(np.sum(t_test, axis=0)/t_test.shape[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### More data checks\n",
    "# Ensure that we have one hot encoding, and that the \n",
    "print(t_vali[:10,:])\n",
    "print(np.where(t_trai == 1)[1][:10])\n",
    "\n",
    "# If you don't get a 10xNUM_CLASS matrix, and a list with the index of the 1 of\n",
    "# each row, something went wrong.\n",
    "\n",
    "\n",
    "### Ensure that the data is scalled appropriately (between 0 and 1)\n",
    "\n",
    "print('\\nMaximum training value: {}'.format(np.max(x_trai)))\n",
    "if np.max(x_trai) > 1:\n",
    "    print('WARNING!! Maximum value in input data is {}'.format(np.max(x_trai)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### VISUALIZE \n",
    "canv, lab = tls.plot_svhn(x_trai, t_trai, t=6, IMG_LEN=IMG_LEN, \n",
    "                          IMG_DEPTH=IMG_DEPTH, cmap=cmap)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.imshow(canv, cmap=cmap, interpolation='nearest')\n",
    "\n",
    "ax.set_title('Data visualization')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(lab) # The labels associated with the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### HYPER PARAMETERS\n",
    "# VOLATILE HP\n",
    "learning_rate = 1e-3\n",
    "smooth_factor=0.01\n",
    "L1 = 0\n",
    "L2 = 0\n",
    "\n",
    "samples_to_process = 1e7\n",
    "val_interval=5e4\n",
    "batch_size = 500\n",
    "\n",
    "if not val_interval/batch_size % 1 == 0:\n",
    "    print('WARNING: val_interval must be divisible by batch_size')\n",
    "    print('Validation will be performed rarely with current settings')\n",
    "\n",
    "\n",
    "# ARCHITECTURE\n",
    "fraction = 0.1\n",
    "hid_size = 128\n",
    "num_latent_1 = round(hid_size*fraction)\n",
    "skip_connections=True\n",
    "deconv_rec=False\n",
    "bernuli_sampling_train=False\n",
    "\n",
    "conv_net_class=True\n",
    "\n",
    "# STABLE HP\n",
    "eq_size = 1\n",
    "iw_size = 1\n",
    "\n",
    "\n",
    "#Prior y\n",
    "prior_y=np.mean(t_trai,axis=0).reshape(1,-1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### HELPER FUNCTIONS\n",
    "from lasagne.objectives import squared_error\n",
    "\n",
    "def onehot(t, num_classes):\n",
    "    out = np.zeros((t.shape[0], num_classes)).astype('float32')\n",
    "    for row, col in enumerate(t):\n",
    "        out[row, col] = 1\n",
    "    return out\n",
    "\n",
    "\n",
    "# c = -0.5 * np.log(2*np.pi)\n",
    "clip = lambda x: T.clip(x,-10,10) #used to limit the variance (why?)\n",
    "\n",
    "def log_bernoulli(x, p, eps=1e-32):\n",
    "    \"\"\"\n",
    "    Computes the binary cross-entropy between a target and \n",
    "\n",
    "    Use eps if you don't want to alow values ==0, ==1\n",
    "    \"\"\"\n",
    "\n",
    "    p = T.clip(p, eps, 1.0 - eps)\n",
    "    return -T.nnet.binary_crossentropy(p, x)\n",
    "\n",
    "\n",
    "\n",
    "def kl_normal_2_stdnormal(mu, lv):\n",
    "    \"\"\"Compute the KL divergence from the standard normal dist\"\"\"\n",
    "    return - 0.5 * (1 + lv - mu**2 - T.exp(lv))\n",
    "\n",
    "\n",
    "def ReconLogLikelihood(mux, x, muq, lvq,KL_weigth=1):\n",
    "    \"\"\"\n",
    "    Compute the cost of the network, using \n",
    "    \"\"\"\n",
    "    #Sum over the latent dimension, mean over the the samples\n",
    "    #reconstruction_cost = -squared_error(x, mux).sum(axis=1).mean()\n",
    "    reconstruction_cost = log_bernoulli(x, mux).sum(axis=1)\n",
    "    #epsilon = 1e-8\n",
    "    #reconstruction_cost =( x * T.log(mux + epsilon) + (1-x)*T.log(1-mux+epsilon))\n",
    "    KL_qp = kl_normal_2_stdnormal(muq, lvq).sum(axis=1)#.mean()\n",
    "    \n",
    "    LL = reconstruction_cost - KL_qp * KL_weigth\n",
    "    \n",
    "    return LL, reconstruction_cost, KL_qp\n",
    "\n",
    "\n",
    "def LogSumExp(x, axis=None, keepdims=False):\n",
    "    ''' Numerically stable theano version of the Log-Sum-Exp trick'''\n",
    "    x_max = T.max(x, axis=axis, keepdims=True)\n",
    "\n",
    "    preres = T.log(T.sum(T.exp(x - x_max), axis=axis, keepdims=keepdims))\n",
    "    return preres + x_max.reshape(preres.shape)\n",
    "\n",
    "def img_transform(x,reverse=False):\n",
    "    if reverse:\n",
    "        x=x.swapaxes(1,3)\n",
    "        x=x.reshape((x.shape[0],IMG_LEN*IMG_LEN*IMG_DEPTH))\n",
    "    else:\n",
    "        x=x.reshape((x.shape[0],IMG_LEN, IMG_LEN, IMG_DEPTH))\n",
    "        x=x.swapaxes(1,3)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "### CREATE MODEL\n",
    "from lasagne.nonlinearities import leaky_rectify, rectify, sigmoid,softmax\n",
    "from parmesan.layers import SampleLayer\n",
    "### CLASSIFIER\n",
    "l_in_x   = L.InputLayer(shape=(None, num_features))#, name='l_in_x')\n",
    "l_in_y   = L.InputLayer(shape=(None, num_classes))#, name='l_in_y')\n",
    "l_in_x_con= L.InputLayer(shape=(None, num_features))#, name='l_in_x')\n",
    "\n",
    "\n",
    "l_cl_1   = L.DenseLayer(l_in_x, \n",
    "                        num_units=hid_size,\n",
    "                        nonlinearity=rectify)#,\n",
    "#                        name='l_cl_1')\n",
    "l_cl_2   = L.DenseLayer(l_cl_1,\n",
    "                        num_units=hid_size,\n",
    "                        nonlinearity=rectify)#,\n",
    "#                        name='l_cl_2')\n",
    "if skip_connections:\n",
    "    l_cl_2=L.ConcatLayer([l_cl_1,l_cl_2])\n",
    "\n",
    "l_y    = L.DenseLayer(l_cl_2, \n",
    "                        num_units=num_classes,\n",
    "                        nonlinearity=softmax)#,\n",
    "#                        name='l_y')\n",
    "    \n",
    "    \n",
    "### ENCODER\n",
    "l_en_con = L.ConcatLayer([l_in_x, l_in_y])#,name='l_en_con')\n",
    "\n",
    "l_en_1   = L.DenseLayer(l_en_con, \n",
    "                        num_units=hid_size,\n",
    "                        nonlinearity=rectify)#,\n",
    "#                        name='l_en_1')\n",
    "\n",
    "l_en_2   = L.DenseLayer(l_en_1,\n",
    "                        num_units=hid_size,\n",
    "                        nonlinearity=rectify)#,\n",
    "#                        name='l_en_2')\n",
    "if skip_connections:\n",
    "    l_en_2=L.ConcatLayer([l_en_1,l_en_2])\n",
    "\n",
    "# Create latent parameters\n",
    "l_mu_1   = L.DenseLayer(l_en_2,\n",
    "                        num_units=num_latent_1,\n",
    "                        nonlinearity=None)#,\n",
    "#                        name='l_mu_1')\n",
    "l_lv_1   = L.DenseLayer(l_en_2,\n",
    "                        num_units=num_latent_1,\n",
    "                        nonlinearity=clip)#,\n",
    "#                        name='l_lv_1')\n",
    "\n",
    "# sample a latent representation:\n",
    "#    z ~ q(z|x) = N(mu(x), logvar(x)\n",
    "l_z_1      = SampleLayer(mean=l_mu_1, \n",
    "                         log_var=l_lv_1, \n",
    "                         eq_samples=eq_size, \n",
    "                         iw_samples=iw_size)#, \n",
    "#                         name='l_z_1')\n",
    "\n",
    "### DECODER\n",
    "\n",
    "l_in_z   = L.InputLayer(shape=(None, num_latent_1))#,\n",
    "#                        name = 'l_in_z')\n",
    "\n",
    "l_dec_con = L.ConcatLayer([l_in_z, l_in_y])#,name='l_en_con')\n",
    "\n",
    "l_dec_1  = L.DenseLayer(l_dec_con, \n",
    "                        num_units = hid_size,\n",
    "                        nonlinearity = rectify)#,\n",
    "#                        name = 'l_dec_1')\n",
    "l_dec_2  = L.DenseLayer(l_dec_1, \n",
    "                        num_units = hid_size,\n",
    "                        nonlinearity = rectify)#,\n",
    "#                       name='l_dec_2')\n",
    "if skip_connections:\n",
    "    l_dec_2=L.ConcatLayer([l_dec_con,l_dec_1,l_dec_2])\n",
    "# Sigmoid is used because the original images are $\\in [0,1]$\n",
    "l_out    = L.DenseLayer(l_dec_2, \n",
    "                        num_units=num_features,\n",
    "                        nonlinearity=sigmoid)#,\n",
    "#                        name='l_out')\n",
    "\n",
    "\n",
    "## CONVULUTIONAL CLASSIFIER\n",
    "l_in_class = L.InputLayer(shape=(None,IMG_DEPTH,IMG_LEN,IMG_LEN)) #note that we use a 4D input since we need to retain the spatial arrangement of the pixels when working with convolutions\n",
    "l_conv_cl_1 = L.Conv2DLayer(l_in_class,num_filters=hid_size//32,filter_size=3 ,pad=(1,1))#32*32*hid_size//32\n",
    "l_conv_cl_1_pool = L.MaxPool2DLayer(l_conv_cl_1,pool_size=(2, 2),stride = 2)\n",
    "l_conv_cl_2 =L.Conv2DLayer(l_conv_cl_1_pool,num_filters=hid_size//16,filter_size=3,pad=(1,1))#16*16*hid_size//16\n",
    "l_conv_cl_2_pool = L.MaxPool2DLayer(l_conv_cl_2,pool_size=(2, 2),stride = 2)\n",
    "l_conv_cl_3 =L.Conv2DLayer(l_conv_cl_2_pool,num_filters=hid_size//8,filter_size=3,pad=(1,1))#8*8*hid_size//8\n",
    "l_conv_cl_3_pool = L.MaxPool2DLayer(l_conv_cl_3,pool_size=(2, 2),stride = 2)\n",
    "l_conv_cl_3 =L.Conv2DLayer(l_conv_cl_2_pool,num_filters=hid_size//4,filter_size=3,pad=(1,1))#4*4*hid_size//4\n",
    "l_conv_cl_dense=L.DenseLayer(l_conv_cl_3, num_units=hid_size//2, nonlinearity=rectify)\n",
    "l_conv_cl_dense=L.DropoutLayer(l_conv_cl_dense)\n",
    "l_out_class = L.DenseLayer(l_conv_cl_dense, num_units=num_classes, nonlinearity=softmax)\n",
    "\n",
    "## CONVULUTIONAL DECODER\n",
    "l_dec_con_in = L.InputLayer(shape=(None,num_latent_1+num_classes, 1, 1))\n",
    "l_dec_con1=L.Deconv2DLayer(l_dec_con_in,num_filters=hid_size//8,filter_size=7)\n",
    "l_dec_con2=L.Deconv2DLayer(l_dec_con1,num_filters=hid_size//16,filter_size=5,stride=2)\n",
    "l_dec_con_out=L.Deconv2DLayer(l_dec_con2,num_filters=IMG_DEPTH,filter_size=5,stride=2,nonlinearity=sigmoid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from lasagne.objectives import categorical_crossentropy, categorical_accuracy\n",
    "##################GENERATING RECONSTRUCTIONS AND CLASSES\n",
    "\n",
    "sym_x = T.matrix('x') # (batch_size x 3072)\n",
    "sym_z = T.matrix('z') # Latent variable (batch_size x num_latent)\n",
    "sym_y = T.matrix('y_l') # Latent variable (batch_size x num_classes)\n",
    "sym_x_l = T.matrix('x_l') # Latent variable (batch_size x 3072)\n",
    "Alpha=0.1\n",
    "Beta=1\n",
    "KL_weigth_uns=1\n",
    "KL_weigth_sup=1\n",
    "\n",
    "\n",
    "####### Repeatition of data for unsupervised learning\n",
    "t_eye = T.eye(num_classes, k=0)\n",
    "t_u = t_eye.reshape((num_classes, 1, num_classes)).repeat(sym_x.shape[0], axis=1).reshape((-1, num_classes))\n",
    "x_u = sym_x.reshape((1, sym_x.shape[0], sym_x.shape[1])).repeat(num_classes, axis=0).reshape((-1, sym_x.shape[1]))\n",
    "#py=prior_y.repeat(sym_x.shape[0])#.reshape(num_classes,sym_x.shape[0]).T\n",
    "#x_u = T.slinalg.kron(sym_x,T.ones((num_classes,1)))\n",
    "py=T.dot(T.ones((sym_x.shape[0],1)),prior_y)\n",
    "\n",
    "def deconv_recon(z_train,t_,deterministic=False):\n",
    "    dec_input_u=T.concatenate((z_train,t_), axis=1)\n",
    "    dec_input_u=dec_input_u.reshape((dec_input_u.shape[0],dec_input_u.shape[1],1,1))\n",
    "    temp_out = L.get_output(l_dec_con_out,{l_dec_con_in:dec_input_u},deterministic = deterministic)\n",
    "    temp_out=temp_out[:,:,2:IMG_LEN+2,2:IMG_LEN+2]\n",
    "    rec_out=img_transform(temp_out,reverse=True)\n",
    "    return rec_out\n",
    "\n",
    "\n",
    "#####################SUPERVISED###################\n",
    "if conv_net_class:\n",
    "    sym_x_l_img=img_transform(sym_x_l)\n",
    "    y_train_l= L.get_output(l_out_class,{l_in_class:sym_x_l_img},deterministic = False)#Classifier\n",
    "else:\n",
    "    y_train_l= L.get_output(l_y,{l_in_x:sym_x_l},deterministic = False)#Classifier\n",
    "\n",
    "z_train_l, mu_train_l, lv_train_l = L.get_output([l_z_1, l_mu_1, l_lv_1],#Encoder\n",
    "                                           {l_in_x:sym_x_l,l_in_y:sym_y}, deterministic = False)\n",
    "if deconv_rec:\n",
    "    recon_train_l=deconv_recon(z_train_l,sym_y)\n",
    "else:\n",
    "    recon_train_l = L.get_output(l_out,{l_in_z:z_train_l,l_in_y:sym_y},deterministic = False)#Decoder\n",
    "\n",
    "#Likelihood\n",
    "LL_rec_train_l, log_px_train_l, KL_train_l = ReconLogLikelihood(recon_train_l, sym_x_l, mu_train_l, lv_train_l,KL_weigth_sup)\n",
    "\n",
    "smooth_y=(1-smooth_factor)*sym_y+smooth_factor*T.dot(T.ones((sym_y.shape[0],1)),prior_y)\n",
    "LL_train_l=T.mean(LL_rec_train_l+Alpha*T.sum(smooth_y*T.log(y_train_l+1e-8)))\n",
    "\n",
    "\n",
    "#############UNSUPERVISED####################\n",
    "if conv_net_class:\n",
    "    sym_x_img=img_transform(sym_x)\n",
    "    y_train_u= L.get_output(l_out_class,{l_in_class:sym_x_img},deterministic = False)#Classifier\n",
    "else:\n",
    "    y_train_u= L.get_output(l_y,{l_in_x:sym_x},deterministic = False)#Classifier\n",
    "\n",
    "z_train_u, mu_train_u, lv_train_u = L.get_output([l_z_1, l_mu_1, l_lv_1],#Encoder\n",
    "                                           {l_in_x:x_u,l_in_y:t_u}, deterministic = False)\n",
    "if deconv_rec:\n",
    "    recon_train_u=deconv_recon(z_train_u,t_u)\n",
    "else:\n",
    "    recon_train_u = L.get_output(l_out,{l_in_z:z_train_u,l_in_y:t_u}, deterministic = False)#Decoder\n",
    "\n",
    "#Likelihood\n",
    "LL_rec_train_u, log_px_train_u, KL_train_u = ReconLogLikelihood(recon_train_u, x_u, mu_train_u, lv_train_u,KL_weigth_uns)\n",
    "LL_rec_train_u = LL_rec_train_u.reshape((num_classes,sym_x.shape[0])).T\n",
    "LL_rec_train_u+=-T.log(y_train_u)*Beta+T.log(py)\n",
    "LL_train_u=T.mean(T.sum(y_train_u*LL_rec_train_u,axis=1))\n",
    "\n",
    "\n",
    "################## EVALUATION##############################\n",
    "if conv_net_class:\n",
    "    sym_x_img=img_transform(sym_x)\n",
    "    y_eval= L.get_output(l_out_class,{l_in_class:sym_x_img},deterministic = True)#Classifier\n",
    "else:\n",
    "    y_eval = L.get_output(l_y,{l_in_x:sym_x},deterministic = True) #Classifier\n",
    "\n",
    "z_eval, mu_eval, lv_eval = L.get_output([l_z_1, l_mu_1, l_lv_1],#Encoder using same split as unsupervised\n",
    "                                           {l_in_x:x_u,l_in_y:t_u}, deterministic = True)\n",
    "if deconv_rec:\n",
    "    recon_eval=deconv_recon(z_eval,t_u,deterministic = True)\n",
    "else:\n",
    "    recon_eval  = L.get_output(l_out,{l_in_z:z_eval,l_in_y:t_u},deterministic = True) #Decoder\n",
    "\n",
    "LL_rec_eval, log_px_eval, KL_eval = ReconLogLikelihood(recon_eval, x_u, mu_eval, lv_eval)\n",
    "LL_rec_eval = LL_rec_eval.reshape((num_classes,sym_x.shape[0])).T\n",
    "\n",
    "LL_eval=T.mean(T.sum(y_eval,axis=1))\n",
    "\n",
    "p = [y_eval.shape,LL_rec_eval.shape]\n",
    "size_f = theano.function([y_eval,LL_rec_eval],p)\n",
    "\n",
    "#########Training likelihood\n",
    "LL_train=LL_train_u+LL_train_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define variables to output\n",
    "#LL_train,LL_train_u,LL_train_l allready defined\n",
    "certainty_class_u=T.mean(T.max(y_train_u,axis=-1))\n",
    "prob_recon_u=T.exp(LL_rec_train_u-LogSumExp(LL_rec_train_u,axis=-1,keepdims=True))\n",
    "certainty_recon_u=T.mean(T.max(prob_recon_u,axis=-1))\n",
    "mean_var_u=T.mean(lv_train_u)\n",
    "recon_class_same_u=categorical_accuracy(prob_recon_u,y_train_u).mean()\n",
    "\n",
    "#supervised outputs\n",
    "certainty_class_l=T.mean(T.max(y_train_l,axis=-1))\n",
    "mean_var_l=T.mean(lv_train_l)\n",
    "\n",
    "#Evaluation output\n",
    "prob_recon_eval=T.exp(LL_rec_eval-LogSumExp(LL_rec_eval,axis=-1,keepdims=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "### CREATE TRAINING FUNCTIONS\n",
    "if conv_net_class:\n",
    "    if deconv_rec:\n",
    "        all_params = L.get_all_params([l_z_1, l_dec_con_out,l_out_class], trainable=True)\n",
    "    else:\n",
    "        all_params = L.get_all_params([l_z_1, l_out,l_out_class], trainable=True)\n",
    "else:\n",
    "    if deconv_rec:\n",
    "        all_params = L.get_all_params([l_z_1, l_dec_con_out,l_y], trainable=True)\n",
    "    else:\n",
    "        all_params = L.get_all_params([l_z_1, l_out,l_y], trainable=True)\n",
    "all_grads  = T.grad(-LL_train, all_params)\n",
    "\n",
    "updates    = lasagne.updates.adam(all_grads, all_params,\n",
    "                                  learning_rate=learning_rate)\n",
    "\n",
    "#Training function: Return loss, and update weights\n",
    "f_train = theano.function(inputs=[sym_x,sym_x_l,sym_y],\n",
    "                          outputs=[LL_train,\n",
    "                                   LL_train_u,\n",
    "                                   LL_train_l,\n",
    "                                   certainty_class_u,\n",
    "                                   mean_var_u,\n",
    "                                   recon_class_same_u,\n",
    "                                   certainty_recon_u,\n",
    "                                   certainty_class_l,\n",
    "                                   mean_var_l,\n",
    "                                   prob_recon_u,\n",
    "                                   y_train_u,LL_rec_train_u],\n",
    "                          updates=updates)\n",
    "\n",
    "# Evaluation function: Return loss\n",
    "f_eval = theano.function(inputs=[sym_x],\n",
    "                         outputs=[LL_eval,y_eval])\n",
    "f_eval_recon = theano.function(inputs=[sym_x],\n",
    "                         outputs=[LL_eval,y_eval,prob_recon_eval])\n",
    "\n",
    "# Get latent variable values\n",
    "f_z = theano.function(inputs=[sym_x], outputs=[z_eval])\n",
    "\n",
    "# Return the reconstruction\n",
    "f_reconstruction = theano.function(inputs=[sym_x], outputs=[recon_eval])\n",
    "\n",
    "# Simulate artificial data, given an artificial latent variable\n",
    "#f_simulate = theano.function(inputs=[sym_z], outputs=[mux_sample])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print()\n",
    "# #test functions\n",
    "# test_y=np.random.randint(num_classes, size=(50)).astype('float32') #dummy data\n",
    "# test_y=onehot(test_y,num_classes)\n",
    "# test_x =np.random.normal(0,1, (200, 32*32*3)).astype('float32') #dummy data\n",
    "# text_x_l=np.random.normal(0,1, (50, 32*32*3)).astype('float32') #dummy data\n",
    "\n",
    "# f_temp = theano.function(inputs=[sym_x,sym_x_l,sym_y],\n",
    "#                           outputs=[LL_train,LL_train_u,LL_train_l,certainty_class_u,mean_var_u,certainty_class_l,mean_var_l,prob_recon_u,y_train_u]#,,\n",
    "# #        prob_recon_u],\n",
    "#                           ,updates=updates)\n",
    "# out=f_temp(test_x,text_x_l,test_y)\n",
    "# #[LL_train,LL_train_u,LL_train_l,certainty_class_u,mean_var_u,recon_class_same_u,certainty_class_l,mean_var_l,\n",
    "# #        prob_recon_u,y_train_u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "from IPython.display import Image, display, clear_output\n",
    "LL_train, KL_train, logpx_train = [],[],[]\n",
    "LL_valid, KL_valid, logpx_valid = [],[],[]\n",
    "samples_processed = 0\n",
    "plt_vals = []\n",
    "plt_vals_t = []\n",
    "samples_processed =0\n",
    "valid_samples_processed = []\n",
    "scalarInputs=9\n",
    "NonScalarInputs=4\n",
    "out_accum=[0]*(scalarInputs+NonScalarInputs)\n",
    "valid_accuracy_class=[]\n",
    "valid_accuracy_recon=[]\n",
    "valid_recon_class_same=[]\n",
    "\n",
    "\n",
    "try:\n",
    "    while samples_processed < samples_to_process:\n",
    "#        print(\"Number of samples processed: {}\".format(samples_processed))\n",
    "        idxs = np.random.choice(range(x_trai.shape[0]), size=(batch_size), replace=False) \n",
    "        x_batch = x_trai[idxs]\n",
    "        t_batch = t_trai[idxs]\n",
    "\n",
    "        if bernuli_sampling_train:\n",
    "            x_batch_ber=np.random.binomial(1,x_batch,size=x_batch.shape).astype(theano.config.floatX)\n",
    "            x_train_l_ber=np.random.binomial(1,x_train_l,size=x_train_l.shape).astype(theano.config.floatX)\n",
    "            out = f_train(x_batch_ber, x_train_l_ber, t_train_l)\n",
    "        else:\n",
    "            out = f_train(x_batch, x_train_l, t_train_l)\n",
    "#        uns_results.add_batch([out,t_batch],batch_size)\n",
    "  \n",
    "        mean_factor=min(0.99,samples_processed/batch_size)\n",
    "        samples_processed += batch_size\n",
    "\n",
    "#        print(out_accum[i],mean_factor,(1-mean_factor),out[i])\n",
    "\n",
    "        for i in range(scalarInputs):\n",
    "            out_accum[i]=out_accum[i]*mean_factor+(1-mean_factor)*out[i]\n",
    "        \n",
    "        plt_vals += [out_accum[0]]\n",
    "        plt_vals_t += [out_accum[4]]\n",
    "\n",
    "        \n",
    "        if samples_processed % val_interval == 0:\n",
    "            valid_samples_processed += [samples_processed]\n",
    "            #size_f(y_eval,LL_rec_eval)\n",
    "\n",
    "            \n",
    "            out = f_eval_recon(x_vali)\n",
    "            LL_valid += [out[0]]\n",
    "            pred_y_class=out[1]\n",
    "            pred_y_recon=out[2]\n",
    "            valid_accuracy_class+=[np.sum(np.equal(np.argmax(pred_y_class,axis=1),np.argmax(t_vali,axis=1)))/(t_vali.shape[0]*1.0)]\n",
    "            valid_accuracy_recon+=[np.sum(np.equal(np.argmax(pred_y_recon,axis=1),np.argmax(t_vali,axis=1)))/(t_vali.shape[0]*1.0)]\n",
    "            valid_recon_class_same+=[np.sum(np.equal(np.argmax(pred_y_recon,axis=1),np.argmax(pred_y_class,axis=1)))/(t_vali.shape[0]*1.0)]\n",
    "\n",
    "            #z_eval = f_z(x_vali)[0]\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            print(\"Samples processed: {} \".format(samples_processed))   \n",
    "            print(\"Accumulated outs: {}\".format(out_accum,LL_valid[-1]))\n",
    "            print(\"Validation accuracies: {}\".format([valid_accuracy_class[-1],valid_accuracy_recon[-1],valid_recon_class_same[-1]]))\n",
    "            t_plots=4\n",
    "            \n",
    "            x_plot=x_vali[np.random.choice(range(x_vali.shape[0]), size=(t_plots*t_plots), replace=False)]\n",
    "            x_plot_recon = f_reconstruction(x_plot)\n",
    "            org_plot=tls.plot_svhn(x_plot, \n",
    "                                             t=t_plots, cmap=cmap, IMG_LEN=IMG_LEN, \n",
    "                                             IMG_DEPTH=IMG_DEPTH,t2=t_plots,choose_random=False)\n",
    "            #print(type(org_plot),org_plot)\n",
    "            _, ax = plt.subplots(1,num_classes+1)\n",
    "            ax[0].imshow(org_plot[0], cmap=cmap, interpolation='None')\n",
    "            ax[0].set_title('Original')\n",
    "            ax[0].axis('off')\n",
    "            x_plot_recon[0]=1-x_plot_recon[0]\n",
    "            x_plot_recon[0]*=255\n",
    "            \n",
    "            for c in range(num_classes):\n",
    "                rec_plot=tls.plot_svhn(x_plot_recon[0][np.array(range(t_plots*t_plots))+(t_plots*t_plots)*c,:].astype('int8'),t=t_plots, cmap=cmap, IMG_LEN=IMG_LEN, \n",
    "                                             IMG_DEPTH=IMG_DEPTH,t2=t_plots,choose_random=False)\n",
    "                ax[c+1].imshow(rec_plot[0], cmap=cmap, interpolation='None')\n",
    "                ax[c+1].set_title('Reconstruction')\n",
    "                ax[c+1].axis('off')\n",
    "\n",
    "                #plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            x_plot=x_train_l[np.random.choice(range(x_train_l.shape[0]), size=(t_plots*t_plots), replace=False)]\n",
    "            x_plot_recon = f_reconstruction(x_plot)\n",
    "            org_plot=tls.plot_svhn(x_plot, \n",
    "                                             t=t_plots, cmap=cmap, IMG_LEN=IMG_LEN, \n",
    "                                             IMG_DEPTH=IMG_DEPTH,t2=t_plots,choose_random=False)\n",
    "            #print(type(org_plot),org_plot)\n",
    "            _, ax = plt.subplots(1,num_classes+1)\n",
    "            ax[0].imshow(org_plot[0], cmap=cmap, interpolation='None')\n",
    "            ax[0].set_title('Original - labeled')\n",
    "            ax[0].axis('off')\n",
    "            x_plot_recon[0]=1-x_plot_recon[0]\n",
    "            x_plot_recon[0]*=255\n",
    "            \n",
    "            for c in range(num_classes):\n",
    "                rec_plot=tls.plot_svhn(x_plot_recon[0][np.array(range(t_plots*t_plots))+(t_plots*t_plots)*c,:].astype('int8'),t=t_plots, cmap=cmap, IMG_LEN=IMG_LEN, \n",
    "                                             IMG_DEPTH=IMG_DEPTH,t2=t_plots,choose_random=False)\n",
    "                ax[c+1].imshow(rec_plot[0], cmap=cmap, interpolation='None')\n",
    "                #ax[c+1].set_title('Reconstruction- labeled')\n",
    "                ax[c+1].axis('off')\n",
    "\n",
    "                #plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_l_ber=np.random.binomial(1,x_train_l,size=x_train_l.shape).astype(theano.config.floatX)\n",
    "org_plot=tls.plot_svhn(x_train_l_ber, \n",
    "                             t=t_plots, cmap=cmap, IMG_LEN=IMG_LEN, \n",
    "                             IMG_DEPTH=IMG_DEPTH,t2=t_plots,choose_random=True)\n",
    "#print(type(org_plot),org_plot)\n",
    "_, ax = plt.subplots(1,num_classes+1)\n",
    "ax[0].imshow(org_plot[0], cmap=cmap, interpolation='None')\n",
    "ax[0].set_title('Original - labeled')\n",
    "ax[0].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = f_eval(x_trai[np.random.choice(range(x_trai.shape[0]), size=(t_plots*t_plots), replace=False)])\n",
    "f_temp = theano.function(inputs=[sym_x],\n",
    "                         outputs=[y_train_u])\n",
    "out=f_temp(x_trai[np.random.choice(range(x_trai.shape[0]), size=(t_plots*t_plots), replace=False)])\n",
    "print(out)\n",
    "#pred_y_class=out[1]\n",
    "#pred_y_recon=out[2]\n",
    "#valid_accuracy_class+=[np.sum(np.equal(np.argmax(pred_y_class,axis=1),np.argmax(t_vali,axis=1)))/t_vali.shape[0]]\n",
    "#valid_accuracy_recon+=[np.sum(np.equal(np.argmax(pred_y_recon,axis=1),np.argmax(t_vali,axis=1)))/t_vali.shape[0]]\n",
    "#valid_recon_class_same+=[np.sum(np.equal(np.argmax(pred_y_recon,axis=1),np.argmax(pred_y_class,axis=1)))/t_vali.shape[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(valid_accuracy_class,valid_accuracy_recon,valid_recon_class_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test=f_eval_recon(x_train_l)\n",
    "print(test[1][0:10])\n",
    "print(t_train_l[0:10])\n",
    "print(np.sum(np.equal(np.argmax(pred_y_class,axis=1),np.argmax(t_vali,axis=1)))/(1.0*t_vali.shape[0]))\n",
    "print(prior_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
